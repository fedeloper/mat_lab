{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Readme: how to handle our Sentinel-1 ASC-DESC dataset\n",
    "\n",
    "###What you have: \n",
    "A list of big geotiff file (.tif) Pairs. Each image in the pair should roughly correspond to the same area as the other. Each geotiff can by read using rasterio or gdal, as you can see in the notebook.\n",
    "\n",
    "###What you should do: \n",
    "create single datapoints to train/test your models.\n",
    "\n",
    "###How to do it: \n",
    "For each big pair, get a datapoint by cropping a random patch from one side (this acts as the \"query\" patch, the one you want to look for) and cropping a patch around the same coordinates from the other side (this acts as the \"search\" patch, the one to want to search in). Add distortions: search patch has to be taken with a random offset around query patch (perhaps random but normally distributed around offset 0, which means query patch at the centre). Also add a bit of random scale and rotation.Try finding a 1km^2 query patch inside a 4km^2 search patch. \n",
    "\n",
    "###Some WARNINGS (not too sure about them)\n",
    "\n",
    "Warning: do not include every correspondence in the ground truth pixel correspondence list! you should only insert meaningful correspondences such as harris corners.\n",
    "\n",
    "Warning: maybe discard a candidate query altogether if it's not feature-rich enough. You can do this using pixel histograms (has to have more than n peaks -> use scikit learn), or maybe entropy. for example, if every pixel is around e.g. 150, this means that everything is gray and unmatchable."
   ],
   "metadata": {
    "id": "MIGuQbmAl3-q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pytorch_lightning as pl\n",
    "import argparse\n",
    "import pprint\n",
    "from loguru import logger as loguru_logger\n",
    "from scipy.ndimage import uniform_filter, variance\n",
    "from skimage.transform import AffineTransform\n",
    "\n",
    "from src.config.default import get_cfg_defaults\n",
    "from src.utils.profiler import build_profiler\n",
    "\n",
    "from src.lightning.lightning_aspanformer import PL_ASpanFormer\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageFilter\n",
    "import findpeaks\n",
    "from torch import optim\n",
    "! pip install rasterio\n",
    "import pymagsac\n",
    "import rasterio as rio\n",
    "from rasterio import warp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "import pytorch_lightning as pl\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "\n",
    "import math\n",
    "import argparse\n",
    "import pprint\n",
    "from distutils.util import strtobool\n",
    "from pathlib import Path\n",
    "from loguru import logger as loguru_logger\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Need this to plot in HD\n",
    "# This takes up a lot of memory!\n",
    "matplotlib.rcParams['figure.dpi'] = 300\n",
    "\n",
    "if 'model_TransSAR' not in locals():\n",
    "    model_TransSAR=None"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xrueqd_5HPm0",
    "outputId": "faa7755a-0d9c-4a33-9488-943c36621724",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_correspondence_multi(src_data, dst_data, src_row, src_col):\n",
    "  ''' Gets the pixel coordinates (dst_row,dst_col) in dst_data of a given pixel at (src_row,src_col) in src_data.\n",
    "      Warning: no handling of pixel being outside dst_data! You have to handle that yourself.\n",
    "      TODO: Should add a check to only get the correspondences with high enough cornerness on both sides\n",
    "      (because we should only insert meaningful correspondences in the list!).\n",
    "  '''\n",
    "  # Get geo coords of pixel from src dataset\n",
    "  X1, Y1 = rio.transform.xy(src_data.transform, src_row, src_col)\n",
    "  # print(\"Geographic coordinates in crs 1: \",X1,Y1)\n",
    "\n",
    "  if src_data.crs != dst_data.crs:\n",
    "    # convert coordinates in the crs of the dst dataset\n",
    "    X2, Y2 = warp.transform(src_data.crs, dst_data.crs, X1, Y1)\n",
    "  else:\n",
    "    # if the crs is the same, do nothing\n",
    "    X2, Y2 = X1, Y1\n",
    "\n",
    "  # Get corresponding px coords in dst dataset\n",
    "  # It still returns an index even if out of bounds\n",
    "  dst_row, dst_col = rio.transform.rowcol(dst_data.transform, X2, Y2)\n",
    "  # print(\"Corresponding pixel coordinates in image 2: \",dst_row,dst_col)\n",
    "  return dst_row, dst_col\n",
    "\n",
    "def get_correspondence(src_data, dst_data ,src_row, src_col):\n",
    "  ''' Gets the pixel coordinates (dst_row,dst_col) in dst_data of a given pixel at (src_row,src_col) in src_data.\n",
    "      Warning: no handling of pixel being outside dst_data! You have to handle that yourself.\n",
    "      TODO: Should add a check to only get the correspondences with high enough cornerness on both sides \n",
    "      (because we should only insert meaningful correspondences in the list!).\n",
    "  '''\n",
    "  # Get geo coords of pixel from src dataset\n",
    "  X1, Y1 = src_data.xy(src_row, src_col)\n",
    "  X1,Y1 = X1 if type(X1) is list else [X1], Y1 if type(Y1) is list else [Y1]\n",
    "  #print(\"Geographic coordinates in crs 1: \",X1,Y1)\n",
    "\n",
    "  if src_data.crs != dst_data.crs:\n",
    "    # convert coordinates in the crs of the dst dataset\n",
    "    X2, Y2 = warp.transform(src_data.crs, dst_data.crs,X1,Y1)\n",
    "    X2 = X2[0]\n",
    "    Y2 = Y2[0]\n",
    "  else: \n",
    "    # if the crs is the same, do nothing\n",
    "    X2, Y2 = X1, Y1\n",
    "\n",
    "  # Get corresponding px coords in dst dataset\n",
    "  # It still returns an index even if out of bounds\n",
    "  dst_row, dst_col = dst_data.index(X2, Y2)\n",
    "  dst_row, dst_col =  dst_row[0] if type(dst_row) is list else dst_row, dst_col[0] if type(dst_col) is list else dst_col\n",
    "  if dst_row < 0 or dst_col <0:\n",
    "      print(\"make the margin higher, the corresponding points are outside the image\")\n",
    "  #print(\"Corresponding pixel coordinates in image 2: \",dst_row,dst_col)\n",
    "  return dst_row , dst_col\n",
    "\n",
    "def get_datapoint(ref_data,query_data):\n",
    "  ''' TODO: Call this method on a pair of rasterio datasets. It will generate a random datapoint consisting\n",
    "      of a smaller SAR patch from the query dataset, and a bigger SAR patch from the reference dataset. \n",
    "      The search patch contains the area of the smaller patch, with a random offset. \n",
    "  '''\n",
    "  datapoint = None\n",
    "  return datapoint\n",
    "\n",
    "def is_good_patch(patch):\n",
    "  ''' TODO: Checks if the random query patch is sufficiently texture-rich to be used to train/test the matching model.\n",
    "      If the patch is too plain (e.g. sea or desert), returns False. This function should also be used at test time: \n",
    "      if a sensor image is too plain, there's no need to match it. \n",
    "      At test time, something similar should also be done on the reference side. \n",
    "  '''\n",
    "  good = None\n",
    "  return good\n",
    "\n",
    "def get_keypoints(patch):\n",
    "  ''' TODO: Use something like harris corner detection to get the list of ground truth correspondences.\n",
    "      In fact, you should not train on each pixel corresp, but you should select only meaningful corresp!\n",
    "      Harris peaks might be just strong speckle noise, but if you take strong ones you should be fine.\n",
    "  '''\n",
    "  keypoints = None\n",
    "  return keypoints\n",
    "\n",
    "def normalize_image(img):\n",
    "  ''' Normalize by clipping to 99th percentile and convert to uint8.\n",
    "      This clips strong speckle outliers and optimizes the brightness range\n",
    "  '''\n",
    "  p1 = np.nanpercentile(img,99)\n",
    "  img = img.clip(0,p1)\n",
    "  img = (img-np.nanmin(img))/(np.nanmax(img)-np.nanmin(img))*255\n",
    "  img = img.astype(np.uint8, copy=True)\n",
    "  return img"
   ],
   "metadata": {
    "id": "_IMltSlzD-C0"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "paths = ['./data/paired_sentinel/Sentinel1-AD Dataset- Lat-0.015502064951149919Lon41.3552557262833/S1A_IW_GRDH_1SDV_20220808T060138_20220808T060203_044457_054E18_B16D.tif',\n",
    "         './data/paired_sentinel/Sentinel1-AD Dataset- Lat-0.015502064951149919Lon41.3552557262833/S1A_IW_GRDH_1SDV_20220814T175523_20220814T175548_044552_055140_2AEA.tif']\n",
    "\n",
    "\n",
    "import random\n",
    "def get_sample(tiff1_path,tiff2_path,search_window,patch_size,margin=40,random_seed=1,verbose=True,random_rotation=0.03,random_zoom=0.03):\n",
    "  '''\n",
    "        1-Reads the first band of the TIFF files using the rio library and normalizes the image data.\n",
    "        2-Selects a random search window and patch location within the image using random number generators.\n",
    "        3-Calls the get_correspondence function on the selected locations to find the corresponding locations in the second image.\n",
    "        4-Converts the grayscale images to RGB format using OpenCV.\n",
    "        5-Creates centered patches from the RGB images by zero-padding the images and copying a portion of the original images to the patches.\n",
    "        6-Draws rectangles around the search window and patch in both images.\n",
    "        7-Saves the patch and search window as JPEG files.\n",
    "        8-Returns the processed RGB images, the points of the search window and patch, and the original rio datasets.\n",
    "\n",
    "  '''\n",
    "  dataset1 = rio.open(tiff1_path)\n",
    "  dataset2 = rio.open(tiff2_path)\n",
    "  img1 = dataset1.read(1)\n",
    "  img1 = normalize_image(img1)\n",
    "  img2 = dataset2.read(1)\n",
    "  img2 = normalize_image(img2)\n",
    "\n",
    "  img1= np.swapaxes(img1,0,1)\n",
    "  img2= np.swapaxes(img2,0,1)\n",
    "\n",
    "  search_window_w,search_window_h = search_window\n",
    "  patch_size_w,patch_size_h = patch_size\n",
    "\n",
    "  if img1.shape[0]-search_window_w-margin*2 <0  or img1.shape[1]-search_window_h-margin*2 <0:\n",
    "    print(\"margin + search windows is too big for the image:\",margin,\"*2 +\",(search_window_w,search_window_h),\">\",img1.shape)\n",
    "    return None,None,None,None,None,None,None\n",
    "  lu = ( margin+random.randint(0,img1.shape[0]-search_window_w-margin*2),margin+random.randint(0,img1.shape[1]-search_window_h-margin*2))\n",
    "  lu_patch=( lu[0] + random.randint(0,search_window_w-patch_size_w),lu[1] + random.randint(0,search_window_h-patch_size_h))\n",
    "\n",
    "\n",
    "  points_patch = lu_patch\n",
    "  points = lu\n",
    "  #print(points,\"points\",img1.shape)\n",
    "  points_ref = get_correspondence(dataset1, dataset2,lu[0],lu[1]  )\n",
    "  points_patch_ref = get_correspondence(dataset1, dataset2,lu_patch[0] ,lu_patch[1] )\n",
    "\n",
    "  rgb_img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2RGB)\n",
    "  rgb_img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "  patch_source = np.zeros((search_window_w, search_window_h), dtype = np.uint8)\n",
    "\n",
    "  patch_source[int(search_window_w/2 - patch_size_w/2):int(search_window_w/2 + patch_size_w/2),int(search_window_h/2 - patch_size_h/2):int(search_window_h/2 + patch_size_h/2)] = img1[points_patch[0]:points_patch[0]+patch_size_w,points_patch[1]:points_patch[1]+patch_size_h]\n",
    "\n",
    "\n",
    "  patch_dest = np.zeros((search_window_w, search_window_h), dtype = np.uint8)\n",
    "  patch_dest[int(search_window_w/2 - patch_size_w/2):int(search_window_w/2 + patch_size_w/2),int(search_window_h/2 - patch_size_h/2):int(search_window_h/2 + patch_size_h/2)]=img2[points_patch_ref[0]:points_patch_ref[0]+patch_size_w,points_patch_ref[1]:points_patch_ref[1]+patch_size_h]\n",
    "\n",
    "\n",
    "\n",
    "  search_window_source = img1[points[0]:points[0]+search_window_w,points[1]:points[1]+search_window_h]\n",
    "  search_window_dest = img2[ points_ref[0]:points_ref[0]+search_window_w,points_ref[1] : points_ref[1]+search_window_h]\n",
    "\n",
    "\n",
    "  Image.fromarray(cv2.cvtColor(patch_dest, cv2.COLOR_GRAY2RGB), \"RGB\").save(\"patch.jpeg\")\n",
    "  Image.fromarray(cv2.cvtColor(search_window_source, cv2.COLOR_GRAY2RGB), \"RGB\").save(\"search_window.jpeg\")\n",
    "\n",
    "    #draw searching windows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  rgb_img1  = cv2.rectangle(rgb_img1 , tuple(reversed(points)),(points[1]+search_window_h ,points[0]+search_window_w), (0,0,255), 2)\n",
    "  rgb_img1  = cv2.rectangle(rgb_img1 , tuple(reversed(points_patch)), (points_patch[1]+patch_size_h, points_patch[0]+patch_size_w), (255,0,0), 2)\n",
    "\n",
    "\n",
    "  #draw patch windows\n",
    "  rgb_img2  = cv2.rectangle(rgb_img2 , tuple(reversed(points_ref)), ( points_ref[1]+search_window_h,points_ref[0]+search_window_w), (0,0,255), 2)\n",
    "  rgb_img2  = cv2.rectangle(rgb_img2 , tuple(reversed(points_patch_ref)), ( points_patch_ref[1]+patch_size_h,points_patch_ref[0]+patch_size_w), (255,0,0), 2)\n",
    "\n",
    "  #print(rgb_img1.shape,search_window_source.shape)\n",
    "\n",
    "\n",
    "  rgb_img1= np.swapaxes(rgb_img1,0,1)\n",
    "  rgb_img2= np.swapaxes(rgb_img2,0,1)\n",
    "  search_window_source= np.swapaxes(search_window_source,0,1)\n",
    "  patch_source= np.swapaxes(patch_source,0,1)\n",
    "  search_window_dest= np.swapaxes(search_window_dest,0,1)\n",
    "  patch_dest= np.swapaxes(patch_dest,0,1)\n",
    "  if verbose:\n",
    "      fig, axes = plt.subplots(2, 3)\n",
    "      axes[0, 0].set_title('source image')\n",
    "      print(rgb_img1.shape)\n",
    "      axes[0, 0].imshow(PIL.ImageOps.invert(  Image.fromarray(rgb_img1)))\n",
    "\n",
    "\n",
    "      axes[0, 1].set_title('search window source')\n",
    "      axes[0, 1].imshow(PIL.ImageOps.invert(  Image.fromarray( cv2.cvtColor(search_window_source, cv2.COLOR_GRAY2RGB))))\n",
    "\n",
    "      axes[0, 2].set_title('patch source')\n",
    "      axes[0, 2].imshow(PIL.ImageOps.invert(  Image.fromarray(cv2.cvtColor(patch_source, cv2.COLOR_GRAY2RGB))))\n",
    "\n",
    "      axes[1, 0].set_title('dest image')\n",
    "      axes[1, 0].imshow(PIL.ImageOps.invert(  Image.fromarray(rgb_img2)))\n",
    "\n",
    "      axes[1, 1].set_title('search window dest')\n",
    "      axes[1, 1].imshow(PIL.ImageOps.invert(  Image.fromarray(cv2.cvtColor(search_window_dest, cv2.COLOR_GRAY2RGB))))\n",
    "\n",
    "      axes[1, 2].set_title('patch dest')\n",
    "      axes[1, 2].imshow(PIL.ImageOps.invert(  Image.fromarray(cv2.cvtColor(patch_dest, cv2.COLOR_GRAY2RGB))))\n",
    "\n",
    "      plt.show()\n",
    "  rgb_img1= np.swapaxes(rgb_img1,0,1)\n",
    "  rgb_img2= np.swapaxes(rgb_img2,0,1)\n",
    "\n",
    "  return rgb_img1, rgb_img2, points,points_patch_ref,points_patch,dataset1,dataset2\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # init a costum parser which will be added into pl.Trainer parser\n",
    "    # check documentation: https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        'data_cfg_path', type=str,default=\".\", help='data config path')\n",
    "    parser.add_argument(\n",
    "        '--ckpt_path', type=str, default=\"weights/indoor_large-SEA.ckpt\", help='path to the checkpoint')\n",
    "    parser.add_argument(\n",
    "        '--dump_dir', type=str, default=None, help=\"if set, the matching results will be dump to dump_dir\")\n",
    "    parser.add_argument(\n",
    "        '--profiler_name', type=str, default='inference', help='options: [inference, pytorch], or leave it unset')\n",
    "    parser.add_argument(\n",
    "        '--batch_size', type=int, default=1, help='batch_size per gpu')\n",
    "    parser.add_argument(\n",
    "        '--num_workers', type=int, default=2)\n",
    "    parser.add_argument(\n",
    "        '--thr', type=float, default=None, help='modify the coarse-level matching threshold.')\n",
    "\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "from plotting import make_matching_figure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_metrics( mkpts0_r, mkpts1_r,points,points_patch,dataset1,dataset2,searching_window_w,searching_window_h,patch_w,patch_h,img0_raw,img1_raw,color,verbose=False):\n",
    "\n",
    "\n",
    "\n",
    "                # Extract x and y coordinates from mkpts0\n",
    "                x0, y0 = zip(*mkpts0_r)\n",
    "\n",
    "                # Apply translation to the coordinates\n",
    "                x_conv, y_conv = get_correspondence_multi(dataset1, dataset2, [x+points[1] for x in x0], [y+points[0] for y in y0])\n",
    "\n",
    "                # Convert the translated coordinates back to tuples\n",
    "                mk0 = list(zip(x_conv, y_conv))\n",
    "\n",
    "                # Calculate RMSE\n",
    "                rmse = 0\n",
    "                for (x1, y1), (x2, y2) in zip(mk0, mkpts1_r):\n",
    "                    y2 -= (searching_window_w/2)-(patch_w/2)\n",
    "                    x2 -= (searching_window_h/2)-(patch_h/2)\n",
    "                    y2 += points_patch[0]\n",
    "                    x2 += points_patch[1]\n",
    "\n",
    "                    rmse += ((x1-x2)**2 + (y1-y2)**2)\n",
    "\n",
    "                # Normalize the errors by the number of keypoints\n",
    "                num_kpts = len(mk0)\n",
    "                rmse=  (rmse/num_kpts)**(1/2) # like in \"A Transformer-Based Coarse-to-Fine Wide-Swath SAR Image Registration Method under Weak Texture Conditions\"\n",
    "\n",
    "                return rmse\n",
    "\n",
    "def predict_and_print(rgb_img1,rgb_img2,matcher,img0_raw,img1_raw,points,points_patch,dataset1,dataset2,searching_window_w,searching_window_h,patch_w,patch_h,verbose=True,experiment=False,configs_ransac=None ):\n",
    "    '''\n",
    "\n",
    "        This code performs an image matching task with the given matcher, which takes two raw images and outputs corresponding features. The code first resizes the two raw images to (640, 480) and converts them to torch tensors, normalizing them by dividing each pixel by 255. The two images are then passed to the matcher to get feature matches and confidence scores.\n",
    "        The code then computes two weighted average points based on the matches, one weighted by the confidence score and the other by a uniform weight. If the number of matches is greater than 0, the code returns the weighted average points and prints them as figures if verbose is set to True. The output figures are saved as \"LoFTR-colab-demo.pdf\".\n",
    "I       f there are no matches, the code returns None, None, None, None.\n",
    "'''\n",
    "\n",
    "\n",
    "    img0 = torch.from_numpy(img0_raw)[None][None].cuda().to(torch.float) / 255.\n",
    "    img1 = torch.from_numpy(img1_raw)[None][None].cuda().to(torch.float) / 255.\n",
    "\n",
    "    batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "    # Inference with LoFTR and get prediction\n",
    "    with torch.no_grad():\n",
    "\n",
    "            matcher.matcher(batch)\n",
    "            mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "            mkpts1= batch['mkpts1_f'].cpu().numpy()\n",
    "            mconf = batch['mconf'].cpu().numpy()\n",
    "\n",
    "            #mkpts0 = np.array([])\n",
    "\n",
    "    results = []\n",
    "    print()\n",
    "    if mkpts0.shape[0]> 4:\n",
    "\n",
    "        if configs_ransac is not None:\n",
    "\n",
    "            for conf in configs_ransac:\n",
    "                from skimage.measure import ransac\n",
    "                if conf['residual_threshold']>= 0:\n",
    "                    model, inliers = ransac((mkpts0, mkpts1),AffineTransform, min_samples=conf['min_samples'], residual_threshold=conf['residual_threshold'], max_trials=conf['max_trials'])\n",
    "                    n_inliers = np.sum(inliers)\n",
    "                    if n_inliers is None or n_inliers< 3:\n",
    "                        conf.update({'rmse':-1,'inliers':0 if n_inliers is None else n_inliers})\n",
    "                       # results.append({'rmse':-1,'inliers':})\n",
    "                    else:\n",
    "                        rmse = get_metrics(mkpts0[inliers], mkpts1[inliers],points,points_patch,dataset1,dataset2,searching_window_w,searching_window_h,patch_w,patch_h,rgb_img1,rgb_img2,mconf[inliers],verbose=verbose)\n",
    "                else:\n",
    "                     rmse= get_metrics(mkpts0, mkpts1,points,points_patch,dataset1,dataset2,searching_window_w,searching_window_h,patch_w,patch_h,rgb_img1,rgb_img2,mconf,verbose=verbose)\n",
    "                     n_inliers = mkpts0.shape[0]\n",
    "                conf.update({'rmse':rmse,'inliers':n_inliers})\n",
    "                results.append(conf)\n",
    "                if conf['residual_threshold'] == 1:\n",
    "\n",
    "                    color = cm.jet(mconf, alpha=0.7)\n",
    "                    text = [\n",
    "                        'LoFTR',\n",
    "                        'Matches: {}'.format(len(mkpts0)),\n",
    "                    ]\n",
    "                    if verbose:\n",
    "                        #rgb_img1= np.swapaxes(rgb_img1,0,1)\n",
    "                        #rgb_img2= np.swapaxes(rgb_img2,0,1)\n",
    "                        abs_m0 = np.array([(x+points[1],y+points[0]) for x,y in mkpts0[inliers]])\n",
    "                        abs_m1 = np.array([(x-((searching_window_h/2)-(patch_h/2))+points_patch[1] ,y-((searching_window_w/2)-(patch_w/2))+points_patch[0]) for x,y in mkpts1[inliers]])\n",
    "                        fig = make_matching_figure(rgb_img1, rgb_img2, abs_m0, abs_m1, color[inliers], abs_m0, abs_m1, text)\n",
    "                        make_matching_figure(rgb_img1, rgb_img2, abs_m0, abs_m1, color[inliers], abs_m0, abs_m1, text, path=\"LoFTR-colab-demo.pdf\")\n",
    "\n",
    "                        fig = make_matching_figure(img0_raw, img1_raw, mkpts0[inliers], mkpts1[inliers], color[inliers], mkpts0[inliers], mkpts1[inliers], text)\n",
    "                        make_matching_figure(img0_raw, img1_raw, mkpts0[inliers], mkpts1[inliers], color[inliers], mkpts0[inliers], mkpts1[inliers], text, path=\"LoFTR-colab-demo.pdf\")\n",
    "        return results\n",
    "    return None\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "from torchvision.transforms import functional as F\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "\n",
    "loaddirec = \"model.pth\"\n",
    "save_path = \"./\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "'''\n",
    " It takes two images as input, performs image denoising, and then uses a matcher to find the similarity between a patch from the first image and the second image.\n",
    "\n",
    "The code uses a loop to run the processing for 50 times, for each iteration:\n",
    "\n",
    "It calls the function get_sample to get the images, points, and datasets.\n",
    "It denoises the raw images using 3 denoising methods (mean, bilateral, and lee_enhanced). If the denoising didn't produce a result, the code skips to the next iteration.\n",
    "The code calls the function predict_and_print to get the result of the matcher and to find the similarity between the patch and the search window.\n",
    "The code uses the result of the matcher to calculate the error in meters between the predicted position and the real position.\n",
    "The code draws circles on the original full map to show the predicted and real positions.\n",
    "The code displays the original full map if verbose is set to True.\n",
    "The code accumulates the error for each iteration and prints the final result, which includes the number of successful predictions and the mean error in meters.\n",
    "'''\n",
    "def lee_filter(img, size):\n",
    "    img_mean = uniform_filter(img, (size, size))\n",
    "    img_sqr_mean = uniform_filter(img**2, (size, size))\n",
    "    img_variance = img_sqr_mean - img_mean**2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "    return img_output\n",
    "def get_smoothed(file,savefile):\n",
    "    im_file = file\n",
    "    img = cv2.imread(im_file,0)\n",
    "\n",
    "    #noisy_im = (np.float32(img)+1.0)/256.0\n",
    "\n",
    "    #x = np.float32(noisy_im)\n",
    "    #x = F.to_tensor(x)\n",
    "    #x = x.unsqueeze(0)\n",
    "\n",
    "\n",
    "    #pred_im = model_TransSAR(x)\n",
    "    #tmp = pred_im.detach().cpu().numpy()\n",
    "\n",
    "    #tmp = tmp.squeeze()\n",
    "    #tmp = tmp*256 -1\n",
    "\n",
    "    filepath_out = savefile\n",
    "\n",
    "    cv2.imwrite(filepath_out,img)\n",
    "\n",
    "def do_test(matcher_in,path0,path1,size_search=(640, 480),size_patch=(int(180*1.333333), int(180)),configs_ransac=None,verbose=True):\n",
    "\n",
    "\n",
    "  rgb_img1, rgb_img2, points,points_patch_ref,points_patch,dataset1,dataset2 = get_sample(path0,path1,size_search,size_patch,verbose=verbose)\n",
    "  if rgb_img1 is None:\n",
    "      return None\n",
    "  img0_pth = \"./search_window.jpeg\"\n",
    "  img1_pth = \"./patch.jpeg\"\n",
    "  get_smoothed(\"search_window.jpeg\",\"search_window_s.jpeg\")\n",
    "  get_smoothed(\"patch.jpeg\",\"patch_s.jpeg\")\n",
    "  image_pair = [\"search_window_s.jpeg\", \"patch_s.jpeg\"]\n",
    "  img0_raw = cv2.imread(image_pair[0], cv2.IMREAD_GRAYSCALE)\n",
    "  img1_raw = cv2.imread(image_pair[1], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "  img0_denoised =img0_raw\n",
    "  img1_denoised = img1_raw\n",
    "\n",
    "  results = predict_and_print(rgb_img1,rgb_img2,matcher_in,img0_denoised,img1_denoised,points,points_patch,dataset1,dataset2, size_search[0],size_search[1],size_patch[0],size_patch[1],verbose=verbose,configs_ransac=configs_ransac)\n",
    "\n",
    "  return results\n",
    "\n",
    "def get_list():\n",
    "    path_of_the_directory= './data/paired_sentinel/'\n",
    "    paths = []\n",
    "    for filename in os.listdir(path_of_the_directory):\n",
    "        f = os.path.join(path_of_the_directory,filename)\n",
    "        if not os.path.isfile(f):\n",
    "            lst = os.listdir(f)\n",
    "            if len(lst)>1:\n",
    "                paths.append((os.path.join(path_of_the_directory,filename,lst[0]),os.path.join(path_of_the_directory,filename,lst[1])))\n",
    "    return paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./weights/outdoor.ckpt\n",
      "load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 14:29:12.153 | INFO     | src.lightning.lightning_aspanformer:__init__:52 - Load './weights/outdoor.ckpt' as pretrained checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['loftr_coarse.layers.0.attention.sample_offset', 'loftr_coarse.layers.1.attention.sample_offset', 'loftr_coarse.layers.2.attention.sample_offset', 'loftr_coarse.layers.3.attention.sample_offset'], unexpected_keys=[])\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 19.853556408774285, 'inliers': 10}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 2.4260047077259546, 'inliers': 7}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 3.08299769036945, 'inliers': 9}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 3.08299769036945, 'inliers': 9}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 19.853556408774285, 'inliers': 10}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 19.853556408774285, 'inliers': 10}\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 4.705559379687777, 'inliers': 8}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 4.918402602339935, 'inliers': 4}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 4.507927140680133, 'inliers': 5}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 4.705559379687777, 'inliers': 8}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.705559379687777, 'inliers': 8}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 4.705559379687777, 'inliers': 8}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 3.592152472360331, 'inliers': 48}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 2.695861115575333, 'inliers': 24}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 3.121267265634724, 'inliers': 37}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 3.4141006811043826, 'inliers': 45}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 3.592152472360331, 'inliers': 48}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 3.592152472360331, 'inliers': 48}\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 9.75363366496667, 'inliers': 78}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 6.740119651628444, 'inliers': 27}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 6.16700288555636, 'inliers': 66}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 6.099603040076806, 'inliers': 72}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 6.329452792579508, 'inliers': 77}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 6.329452792579508, 'inliers': 77}\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 5.162261847351712, 'inliers': 173}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 4.4552515587364665, 'inliers': 47}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 4.597256812712142, 'inliers': 118}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 4.619245047488739, 'inliers': 147}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.6319444316291705, 'inliers': 171}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 5.162261847351712, 'inliers': 173}\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 17.90925695944498, 'inliers': 40}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 2.7446707436078777, 'inliers': 14}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 2.5465737345512616, 'inliers': 27}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 2.9649992226748845, 'inliers': 29}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.912695560218835, 'inliers': 35}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 18.347652106305738, 'inliers': 38}\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 4.826913396251982, 'inliers': 300}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 3.8146797853854575, 'inliers': 89}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 4.232757145906659, 'inliers': 233}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 4.451277020635633, 'inliers': 288}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.5289930863385495, 'inliers': 299}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 4.826913396251982, 'inliers': 300}\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 16.313202989705857, 'inliers': 29}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 1.9368506142795825, 'inliers': 11}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 3.557131831329126, 'inliers': 18}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 3.7751985552105336, 'inliers': 25}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.1451408490101285, 'inliers': 27}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 6.048375572334936, 'inliers': 28}\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 7.242411886871844, 'inliers': 123}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 3.06852235273402, 'inliers': 28}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 3.2668492412725394, 'inliers': 70}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 3.7659224613543025, 'inliers': 103}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.196167598582166, 'inliers': 121}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 5.5376668202158035, 'inliers': 122}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 7.275292348011438, 'inliers': 199}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 2.3794530572506223, 'inliers': 73}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 2.8381956951460037, 'inliers': 183}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 3.0102001852731903, 'inliers': 188}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 3.7029409911638425, 'inliers': 193}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 7.275292348011438, 'inliers': 199}\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 70.5995874553649, 'inliers': 5}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 78.89399123486261, 'inliers': 4}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 70.5995874553649, 'inliers': 5}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 70.5995874553649, 'inliers': 5}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 70.5995874553649, 'inliers': 5}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 70.5995874553649, 'inliers': 5}\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 5.873030416143729, 'inliers': 75}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 2.916179336186334, 'inliers': 32}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 3.27620246201485, 'inliers': 69}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 3.288024386175263, 'inliers': 71}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 3.4319046229891152, 'inliers': 74}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 5.873030416143729, 'inliers': 75}\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 32.511412626531886, 'inliers': 12}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 9.686334139486975, 'inliers': 6}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 9.030908875328464, 'inliers': 7}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 9.030908875328464, 'inliers': 7}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 28.108794690697653, 'inliers': 8}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 12.424022222791276, 'inliers': 10}\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 24.019850048851108, 'inliers': 36}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 4.442047009226018, 'inliers': 10}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 3.980405743923166, 'inliers': 20}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 4.322652768948993, 'inliers': 28}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.847890395959967, 'inliers': 33}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 4.847890395959967, 'inliers': 33}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 3.982876640643555, 'inliers': 71}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 1.8325760434306895, 'inliers': 20}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 2.398755707873369, 'inliers': 57}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 2.815244906025028, 'inliers': 64}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 3.982876640643555, 'inliers': 71}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 3.982876640643555, 'inliers': 71}\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 3.8407302234105165, 'inliers': 28}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 2.134008595946643, 'inliers': 12}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 2.6584119152909595, 'inliers': 24}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 3.8407302234105165, 'inliers': 28}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 3.8407302234105165, 'inliers': 28}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 3.8407302234105165, 'inliers': 28}\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 29.612186977278824, 'inliers': 70}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 26.420264413264675, 'inliers': 22}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 26.41995427405053, 'inliers': 57}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 26.642968743426277, 'inliers': 66}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 26.63231188838971, 'inliers': 67}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 26.883260551976008, 'inliers': 69}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 57.9548402403953, 'inliers': 10}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 28.202615987361202, 'inliers': 8}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 27.768235211865264, 'inliers': 9}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 27.768235211865264, 'inliers': 9}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 27.768235211865264, 'inliers': 9}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 27.768235211865264, 'inliers': 9}\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 3.3966541078278936, 'inliers': 25}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 3.4792210519273783, 'inliers': 11}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 3.5977608112136084, 'inliers': 19}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 3.3966541078278936, 'inliers': 25}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 3.3966541078278936, 'inliers': 25}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 3.3966541078278936, 'inliers': 25}\n",
      "\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 3.517419284846996, 'inliers': 146}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 2.483587323601683, 'inliers': 63}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 2.9858961024597526, 'inliers': 115}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 3.323129273948344, 'inliers': 139}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 3.517419284846996, 'inliers': 146}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 3.517419284846996, 'inliers': 146}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 3.6981167870569305, 'inliers': 166}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 1.6926265538773537, 'inliers': 68}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 2.1085441153481645, 'inliers': 150}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 2.4893802290808384, 'inliers': 155}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 3.4889968945219807, 'inliers': 165}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 3.6981167870569305, 'inliers': 166}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   min_samples  residual_threshold  max_trials       rmse    inliers  \\\n",
      "0            0                  -1           0  15.982902  78.666667   \n",
      "1            4                   1       10000   9.398251  27.619048   \n",
      "2            4                   3       10000   9.178220  61.809524   \n",
      "3            4                   5       10000   9.400315  71.952381   \n",
      "4            4                  10       10000  11.438762  77.142857   \n",
      "5            4                  20       10000  11.833843  78.095238   \n",
      "\n",
      "   Accepted_match  total_match  \n",
      "0              21           64  \n",
      "1              21           64  \n",
      "2              21           64  \n",
      "3              21           64  \n",
      "4              21           64  \n",
      "5              21           64  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "matcher_in=get_matcher(\"outdoor\",unlock=True)\n",
    "verbose=False\n",
    "random.seed(9)\n",
    "count_yes= 0\n",
    "test_for_each_pair=1\n",
    "configs_ransac=[{'min_samples':0,'residual_threshold':-1,'max_trials':0},\n",
    "                {'min_samples':4,'residual_threshold':1,'max_trials':10000},\n",
    "                      {'min_samples':4,'residual_threshold':3,'max_trials':10000},\n",
    "                      {'min_samples':4,'residual_threshold':5,'max_trials':10000},\n",
    "                      {'min_samples':4,'residual_threshold':10,'max_trials':10000},\n",
    "                      {'min_samples':4,'residual_threshold':20,'max_trials':10000}]\n",
    "paths = get_list()\n",
    "\n",
    "metrics = [{'rmse':0,'inliers':0,'accepted_match':0} for conf in range(len(configs_ransac))]\n",
    "\n",
    "for path0,path1 in paths:\n",
    "    for x in range(test_for_each_pair):\n",
    "              if random.randint(0, 1):\n",
    "                results = do_test(matcher_in,path0,path1,size_search=(640, 480),size_patch=(int(180*1.333333)*2, int(180)*2),verbose=verbose,configs_ransac=configs_ransac)\n",
    "              else:\n",
    "                results = do_test(matcher_in,path1,path0,size_search=(640, 480),size_patch=(int(180*1.333333)*2, int(180)*2),verbose=verbose,configs_ransac=configs_ransac)\n",
    "\n",
    "              if results is not None:\n",
    "\n",
    "                for i in range(len(results)):\n",
    "                    if metrics[i]['rmse']>=0:\n",
    "\n",
    "                        if results[i]['inliers'] is not None:\n",
    "                            metrics[i]['rmse'] += results[i]['rmse']\n",
    "                            metrics[i]['inliers'] += results[i]['inliers']\n",
    "                            metrics[i]['accepted_match'] +=1\n",
    "                    print(results[i])\n",
    "\n",
    "                count_yes+=1\n",
    "\n",
    "data = []\n",
    "for i in range(len(metrics)):\n",
    "        metrics[i]['rmse'] /= metrics[i]['accepted_match']\n",
    "        metrics[i]['inliers'] /= metrics[i]['accepted_match']\n",
    "\n",
    "        configs_ransac[i]['rmse'] = metrics[i]['rmse']\n",
    "        configs_ransac[i]['inliers'] = metrics[i]['inliers']\n",
    "        configs_ransac[i]['Accepted_match'] = metrics[i]['accepted_match']\n",
    "        configs_ransac[i]['total_match'] = test_for_each_pair*len(paths)\n",
    "        data.append( configs_ransac[i])\n",
    "            #print(\"Configuration:\",configs_ransac[i-1],\" Metrics:\",metrics[i], \" Accepted_match:\", metrics[i]['accepted_match'] ,\"/\", test_for_each_pair*len(paths))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "print (df)\n",
    "\n",
    "df.to_excel('players.xlsx')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_samples  residual_threshold  max_trials       rmse    inliers  \\\n",
      "0            0                  -1           0  15.982902  78.666667   \n",
      "1            4                   1       10000   9.398251  27.619048   \n",
      "2            4                   3       10000   9.178220  61.809524   \n",
      "3            4                   5       10000   9.400315  71.952381   \n",
      "4            4                  10       10000  11.438762  77.142857   \n",
      "5            4                  20       10000  11.833843  78.095238   \n",
      "\n",
      "   Accepted_match  total_match  \n",
      "0              21           64  \n",
      "1              21           64  \n",
      "2              21           64  \n",
      "3              21           64  \n",
      "4              21           64  \n",
      "5              21           64  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "print (df)\n",
    "\n",
    "df.to_excel('players.xlsx')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
