{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Readme: how to handle our Sentinel-1 ASC-DESC dataset\n",
    "\n",
    "###What you have: \n",
    "A list of big geotiff file (.tif) Pairs. Each image in the pair should roughly correspond to the same area as the other. Each geotiff can by read using rasterio or gdal, as you can see in the notebook.\n",
    "\n",
    "###What you should do: \n",
    "create single datapoints to train/test your models.\n",
    "\n",
    "###How to do it: \n",
    "For each big pair, get a datapoint by cropping a random patch from one side (this acts as the \"query\" patch, the one you want to look for) and cropping a patch around the same coordinates from the other side (this acts as the \"search\" patch, the one to want to search in). Add distortions: search patch has to be taken with a random offset around query patch (perhaps random but normally distributed around offset 0, which means query patch at the centre). Also add a bit of random scale and rotation.Try finding a 1km^2 query patch inside a 4km^2 search patch. \n",
    "\n",
    "###Some WARNINGS (not too sure about them)\n",
    "\n",
    "Warning: do not include every correspondence in the ground truth pixel correspondence list! you should only insert meaningful correspondences such as harris corners.\n",
    "\n",
    "Warning: maybe discard a candidate query altogether if it's not feature-rich enough. You can do this using pixel histograms (has to have more than n peaks -> use scikit learn), or maybe entropy. for example, if every pixel is around e.g. 150, this means that everything is gray and unmatchable."
   ],
   "metadata": {
    "id": "MIGuQbmAl3-q"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from scipy.ndimage import uniform_filter, variance\n",
    "from skimage.transform import AffineTransform\n",
    "\n",
    "from config.defaultmf import get_cfg_defaults\n",
    "from model.lightning_loftr import PL_LoFTR\n",
    "\n",
    "import rasterio as rio\n",
    "from rasterio import warp\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import argparse\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Need this to plot in HD\n",
    "# This takes up a lot of memory!\n",
    "matplotlib.rcParams['figure.dpi'] = 300\n",
    "\n",
    "if 'model_TransSAR' not in locals():\n",
    "    model_TransSAR = None"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xrueqd_5HPm0",
    "outputId": "faa7755a-0d9c-4a33-9488-943c36621724"
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 16:22:44.971261: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-21 16:22:46.790657: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 16:22:46.790898: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-21 16:22:46.790905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rasterio in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (1.3.5.post1)\r\n",
      "Requirement already satisfied: click-plugins in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (1.1.1)\r\n",
      "Requirement already satisfied: click>=4.0 in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (8.1.3)\r\n",
      "Requirement already satisfied: attrs in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (22.2.0)\r\n",
      "Requirement already satisfied: setuptools in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (65.6.3)\r\n",
      "Requirement already satisfied: cligj>=0.5 in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (0.7.2)\r\n",
      "Requirement already satisfied: certifi in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (2022.12.7)\r\n",
      "Requirement already satisfied: snuggs>=1.4.1 in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (1.4.7)\r\n",
      "Requirement already satisfied: numpy>=1.18 in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (1.23.4)\r\n",
      "Requirement already satisfied: affine in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (2.4.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_correspondence_multi(src_data, dst_data, src_row, src_col):\n",
    "    ''' Gets the pixel coordinates (dst_row,dst_col) in dst_data of a given pixel at (src_row,src_col) in src_data.\n",
    "        Warning: no handling of pixel being outside dst_data! You have to handle that yourself.\n",
    "        TODO: Should add a check to only get the correspondences with high enough cornerness on both sides\n",
    "        (because we should only insert meaningful correspondences in the list!).\n",
    "    '''\n",
    "    # Get geo coords of pixel from src dataset\n",
    "    X1, Y1 = rio.transform.xy(src_data.transform, src_row, src_col)\n",
    "    # print(\"Geographic coordinates in crs 1: \",X1,Y1)\n",
    "\n",
    "    if src_data.crs != dst_data.crs:\n",
    "        # convert coordinates in the crs of the dst dataset\n",
    "        X2, Y2 = warp.transform(src_data.crs, dst_data.crs, X1, Y1)\n",
    "    else:\n",
    "        # if the crs is the same, do nothing\n",
    "        X2, Y2 = X1, Y1\n",
    "\n",
    "    # Get corresponding px coords in dst dataset\n",
    "    # It still returns an index even if out of bounds\n",
    "    dst_row, dst_col = rio.transform.rowcol(dst_data.transform, X2, Y2)\n",
    "    # print(\"Corresponding pixel coordinates in image 2: \",dst_row,dst_col)\n",
    "    return dst_row, dst_col\n",
    "\n",
    "\n",
    "def get_correspondence(src_data, dst_data, src_row, src_col):\n",
    "    ''' Gets the pixel coordinates (dst_row,dst_col) in dst_data of a given pixel at (src_row,src_col) in src_data.\n",
    "        Warning: no handling of pixel being outside dst_data! You have to handle that yourself.\n",
    "        TODO: Should add a check to only get the correspondences with high enough cornerness on both sides\n",
    "        (because we should only insert meaningful correspondences in the list!).\n",
    "    '''\n",
    "    # Get geo coords of pixel from src dataset\n",
    "    X1, Y1 = src_data.xy(src_row, src_col)\n",
    "    X1, Y1 = X1 if type(X1) is list else [X1], Y1 if type(Y1) is list else [Y1]\n",
    "    #print(\"Geographic coordinates in crs 1: \",X1,Y1)\n",
    "\n",
    "    if src_data.crs != dst_data.crs:\n",
    "        # convert coordinates in the crs of the dst dataset\n",
    "        X2, Y2 = warp.transform(src_data.crs, dst_data.crs, X1, Y1)\n",
    "        X2 = X2[0]\n",
    "        Y2 = Y2[0]\n",
    "    else:\n",
    "        # if the crs is the same, do nothing\n",
    "        X2, Y2 = X1, Y1\n",
    "\n",
    "    # Get corresponding px coords in dst dataset\n",
    "    # It still returns an index even if out of bounds\n",
    "    dst_row, dst_col = dst_data.index(X2, Y2)\n",
    "    dst_row, dst_col = dst_row[0] if type(dst_row) is list else dst_row, dst_col[0] if type(\n",
    "        dst_col) is list else dst_col\n",
    "    if dst_row < 0 or dst_col < 0:\n",
    "        print(\"make the margin higher, the corresponding points are outside the image\")\n",
    "    #print(\"Corresponding pixel coordinates in image 2: \",dst_row,dst_col)\n",
    "    return dst_row, dst_col\n",
    "\n",
    "\n",
    "def get_datapoint(ref_data, query_data):\n",
    "    ''' TODO: Call this method on a pair of rasterio datasets. It will generate a random datapoint consisting\n",
    "        of a smaller SAR patch from the query dataset, and a bigger SAR patch from the reference dataset.\n",
    "        The search patch contains the area of the smaller patch, with a random offset.\n",
    "    '''\n",
    "    datapoint = None\n",
    "    return datapoint\n",
    "\n",
    "\n",
    "def is_good_patch(patch):\n",
    "    ''' TODO: Checks if the random query patch is sufficiently texture-rich to be used to train/test the matching model.\n",
    "        If the patch is too plain (e.g. sea or desert), returns False. This function should also be used at test time:\n",
    "        if a sensor image is too plain, there's no need to match it.\n",
    "        At test time, something similar should also be done on the reference side.\n",
    "    '''\n",
    "    good = None\n",
    "    return good\n",
    "\n",
    "\n",
    "def get_keypoints(patch):\n",
    "    ''' TODO: Use something like harris corner detection to get the list of ground truth correspondences.\n",
    "        In fact, you should not train on each pixel corresp, but you should select only meaningful corresp!\n",
    "        Harris peaks might be just strong speckle noise, but if you take strong ones you should be fine.\n",
    "    '''\n",
    "    keypoints = None\n",
    "    return keypoints\n",
    "\n",
    "\n",
    "def normalize_image(img):\n",
    "    ''' Normalize by clipping to 99th percentile and convert to uint8.\n",
    "        This clips strong speckle outliers and optimizes the brightness range\n",
    "    '''\n",
    "    p1 = np.nanpercentile(img, 99)\n",
    "    img = img.clip(0, p1)\n",
    "    img = (img - np.nanmin(img)) / (np.nanmax(img) - np.nanmin(img)) * 255\n",
    "    img = img.astype(np.uint8, copy=True)\n",
    "    return img"
   ],
   "metadata": {
    "id": "_IMltSlzD-C0"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "paths = [\n",
    "    './data/paired_sentinel/Sentinel1-AD Dataset- Lat-0.015502064951149919Lon41.3552557262833/S1A_IW_GRDH_1SDV_20220808T060138_20220808T060203_044457_054E18_B16D.tif',\n",
    "    './data/paired_sentinel/Sentinel1-AD Dataset- Lat-0.015502064951149919Lon41.3552557262833/S1A_IW_GRDH_1SDV_20220814T175523_20220814T175548_044552_055140_2AEA.tif']\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def get_sample(tiff1_path, tiff2_path, search_window, patch_size, margin=40, random_seed=1, verbose=True,\n",
    "               random_rotation=0.03, random_zoom=0.03):\n",
    "    '''\n",
    "          1-Reads the first band of the TIFF files using the rio library and normalizes the image data.\n",
    "          2-Selects a random search window and patch location within the image using random number generators.\n",
    "          3-Calls the get_correspondence function on the selected locations to find the corresponding locations in the second image.\n",
    "          4-Converts the grayscale images to RGB format using OpenCV.\n",
    "          5-Creates centered patches from the RGB images by zero-padding the images and copying a portion of the original images to the patches.\n",
    "          6-Draws rectangles around the search window and patch in both images.\n",
    "          7-Saves the patch and search window as JPEG files.\n",
    "          8-Returns the processed RGB images, the points of the search window and patch, and the original rio datasets.\n",
    "\n",
    "    '''\n",
    "    dataset1 = rio.open(tiff1_path)\n",
    "    dataset2 = rio.open(tiff2_path)\n",
    "    img1 = dataset1.read(1)\n",
    "    img1 = normalize_image(img1)\n",
    "    img2 = dataset2.read(1)\n",
    "    img2 = normalize_image(img2)\n",
    "\n",
    "    img1 = np.swapaxes(img1, 0, 1)\n",
    "    img2 = np.swapaxes(img2, 0, 1)\n",
    "\n",
    "    search_window_w, search_window_h = search_window\n",
    "    patch_size_w, patch_size_h = patch_size\n",
    "\n",
    "    if img1.shape[0] - search_window_w - margin * 2 < 0 or img1.shape[1] - search_window_h - margin * 2 < 0:\n",
    "        print(\"margin + search windows is too big for the image:\", margin, \"*2 +\", (search_window_w, search_window_h),\n",
    "              \">\", img1.shape)\n",
    "        return None, None, None, None, None, None, None\n",
    "    lu = (margin + random.randint(0, img1.shape[0] - search_window_w - margin * 2),\n",
    "          margin + random.randint(0, img1.shape[1] - search_window_h - margin * 2))\n",
    "    lu_patch = (lu[0] + random.randint(0, search_window_w - patch_size_w),\n",
    "                lu[1] + random.randint(0, search_window_h - patch_size_h))\n",
    "\n",
    "    points_patch = lu_patch\n",
    "    points = lu\n",
    "    #print(points,\"points\",img1.shape)\n",
    "    points_ref = get_correspondence(dataset1, dataset2, lu[0], lu[1])\n",
    "    points_patch_ref = get_correspondence(dataset1, dataset2, lu_patch[0], lu_patch[1])\n",
    "\n",
    "    rgb_img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2RGB)\n",
    "    rgb_img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    patch_source = np.zeros((search_window_w, search_window_h), dtype=np.uint8)\n",
    "\n",
    "    patch_source[int(search_window_w / 2 - patch_size_w / 2):int(search_window_w / 2 + patch_size_w / 2),\n",
    "    int(search_window_h / 2 - patch_size_h / 2):int(search_window_h / 2 + patch_size_h / 2)] = img1[points_patch[0]:\n",
    "                                                                                                    points_patch[\n",
    "                                                                                                        0] + patch_size_w,\n",
    "                                                                                               points_patch[1]:\n",
    "                                                                                               points_patch[\n",
    "                                                                                                   1] + patch_size_h]\n",
    "\n",
    "    patch_dest = np.zeros((search_window_w, search_window_h), dtype=np.uint8)\n",
    "    patch_dest[int(search_window_w / 2 - patch_size_w / 2):int(search_window_w / 2 + patch_size_w / 2),\n",
    "    int(search_window_h / 2 - patch_size_h / 2):int(search_window_h / 2 + patch_size_h / 2)] = img2[points_patch_ref[0]:\n",
    "                                                                                                    points_patch_ref[\n",
    "                                                                                                        0] + patch_size_w,\n",
    "                                                                                               points_patch_ref[1]:\n",
    "                                                                                               points_patch_ref[\n",
    "                                                                                                   1] + patch_size_h]\n",
    "\n",
    "    search_window_source = img1[points[0]:points[0] + search_window_w, points[1]:points[1] + search_window_h]\n",
    "    search_window_dest = img2[points_ref[0]:points_ref[0] + search_window_w,\n",
    "                         points_ref[1]: points_ref[1] + search_window_h]\n",
    "\n",
    "    Image.fromarray(cv2.cvtColor(patch_dest, cv2.COLOR_GRAY2RGB), \"RGB\").save(\"patch.jpeg\")\n",
    "    Image.fromarray(cv2.cvtColor(search_window_source, cv2.COLOR_GRAY2RGB), \"RGB\").save(\"search_window.jpeg\")\n",
    "\n",
    "    #draw searching windows\n",
    "\n",
    "    rgb_img1 = cv2.rectangle(rgb_img1, tuple(reversed(points)),\n",
    "                             (points[1] + search_window_h, points[0] + search_window_w), (0, 0, 255), 2)\n",
    "    rgb_img1 = cv2.rectangle(rgb_img1, tuple(reversed(points_patch)),\n",
    "                             (points_patch[1] + patch_size_h, points_patch[0] + patch_size_w), (255, 0, 0), 2)\n",
    "\n",
    "    #draw patch windows\n",
    "    rgb_img2 = cv2.rectangle(rgb_img2, tuple(reversed(points_ref)),\n",
    "                             (points_ref[1] + search_window_h, points_ref[0] + search_window_w), (0, 0, 255), 2)\n",
    "    rgb_img2 = cv2.rectangle(rgb_img2, tuple(reversed(points_patch_ref)),\n",
    "                             (points_patch_ref[1] + patch_size_h, points_patch_ref[0] + patch_size_w), (255, 0, 0), 2)\n",
    "\n",
    "    #print(rgb_img1.shape,search_window_source.shape)\n",
    "\n",
    "    rgb_img1 = np.swapaxes(rgb_img1, 0, 1)\n",
    "    rgb_img2 = np.swapaxes(rgb_img2, 0, 1)\n",
    "    search_window_source = np.swapaxes(search_window_source, 0, 1)\n",
    "    patch_source = np.swapaxes(patch_source, 0, 1)\n",
    "    search_window_dest = np.swapaxes(search_window_dest, 0, 1)\n",
    "    patch_dest = np.swapaxes(patch_dest, 0, 1)\n",
    "    if verbose:\n",
    "        fig, axes = plt.subplots(2, 3)\n",
    "        axes[0, 0].set_title('source image')\n",
    "        print(rgb_img1.shape)\n",
    "        axes[0, 0].imshow(PIL.ImageOps.invert(Image.fromarray(rgb_img1)))\n",
    "\n",
    "        axes[0, 1].set_title('search window source')\n",
    "        axes[0, 1].imshow(PIL.ImageOps.invert(Image.fromarray(cv2.cvtColor(search_window_source, cv2.COLOR_GRAY2RGB))))\n",
    "\n",
    "        axes[0, 2].set_title('patch source')\n",
    "        axes[0, 2].imshow(PIL.ImageOps.invert(Image.fromarray(cv2.cvtColor(patch_source, cv2.COLOR_GRAY2RGB))))\n",
    "\n",
    "        axes[1, 0].set_title('dest image')\n",
    "        axes[1, 0].imshow(PIL.ImageOps.invert(Image.fromarray(rgb_img2)))\n",
    "\n",
    "        axes[1, 1].set_title('search window dest')\n",
    "        axes[1, 1].imshow(PIL.ImageOps.invert(Image.fromarray(cv2.cvtColor(search_window_dest, cv2.COLOR_GRAY2RGB))))\n",
    "\n",
    "        axes[1, 2].set_title('patch dest')\n",
    "        axes[1, 2].imshow(PIL.ImageOps.invert(Image.fromarray(cv2.cvtColor(patch_dest, cv2.COLOR_GRAY2RGB))))\n",
    "\n",
    "        plt.show()\n",
    "    rgb_img1 = np.swapaxes(rgb_img1, 0, 1)\n",
    "    rgb_img2 = np.swapaxes(rgb_img2, 0, 1)\n",
    "\n",
    "    return rgb_img1, rgb_img2, points, points_patch_ref, points_patch, dataset1, dataset2\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "def parse_args():\n",
    "    # init a costum parser which will be added into pl.Trainer parser\n",
    "    # check documentation: https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        'data_cfg_path', type=str, default=\".\", help='data config path')\n",
    "    parser.add_argument(\n",
    "        '--ckpt_path', type=str, default=\"weights/indoor_large-SEA.ckpt\", help='path to the checkpoint')\n",
    "    parser.add_argument(\n",
    "        '--dump_dir', type=str, default=None, help=\"if set, the matching results will be dump to dump_dir\")\n",
    "    parser.add_argument(\n",
    "        '--profiler_name', type=str, default='inference', help='options: [inference, pytorch], or leave it unset')\n",
    "    parser.add_argument(\n",
    "        '--batch_size', type=int, default=1, help='batch_size per gpu')\n",
    "    parser.add_argument(\n",
    "        '--num_workers', type=int, default=2)\n",
    "    parser.add_argument(\n",
    "        '--thr', type=float, default=None, help='modify the coarse-level matching threshold.')\n",
    "\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "from plotting import make_matching_figure\n",
    "\n",
    "\n",
    "def get_matcher(image_type, unlock=False):\n",
    "    print(\"hello\")\n",
    "    #args = parse_args()\n",
    "    # init default-cfg and merge it with the main- and data-cfg\n",
    "    config = get_cfg_defaults()\n",
    "    #config.merge_from_file(args.data_cfg_path)\n",
    "    pl.seed_everything(config.TRAINER.SEED)  # reproducibility\n",
    "\n",
    "    # tune when testing\n",
    "    threshold = None\n",
    "    if threshold is not None:\n",
    "        config.LOFTR.MATCH_COARSE.THR = threshold\n",
    "\n",
    "    # lightning module\n",
    "\n",
    "    matcher = PL_LoFTR(config, pretrained_ckpt=\"./weights/outdoor-lite-SEA.ckpt\", dump_dir=\".\")\n",
    "\n",
    "    return matcher.cuda().eval()\n",
    "\n",
    "\n",
    "def get_metrics(mkpts0_r, mkpts1_r, points, points_patch, dataset1, dataset2, searching_window_w, searching_window_h,\n",
    "                patch_w, patch_h, img0_raw, img1_raw, color, verbose=False):\n",
    "    # Extract x and y coordinates from mkpts0\n",
    "    x0, y0 = zip(*mkpts0_r)\n",
    "\n",
    "    # Apply translation to the coordinates\n",
    "    x_conv, y_conv = get_correspondence_multi(dataset1, dataset2, [x + points[1] for x in x0],\n",
    "                                              [y + points[0] for y in y0])\n",
    "\n",
    "    # Convert the translated coordinates back to tuples\n",
    "    mk0 = list(zip(x_conv, y_conv))\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = 0\n",
    "    for (x1, y1), (x2, y2) in zip(mk0, mkpts1_r):\n",
    "        y2 -= (searching_window_w / 2) - (patch_w / 2)\n",
    "        x2 -= (searching_window_h / 2) - (patch_h / 2)\n",
    "        y2 += points_patch[0]\n",
    "        x2 += points_patch[1]\n",
    "\n",
    "        rmse += ((x1 - x2) ** 2 + (y1 - y2) ** 2)\n",
    "\n",
    "    # Normalize the errors by the number of keypoints\n",
    "    num_kpts = len(mk0)\n",
    "    rmse = (rmse / num_kpts) ** (\n",
    "                1 / 2)  # like in \"A Transformer-Based Coarse-to-Fine Wide-Swath SAR Image Registration Method under Weak Texture Conditions\"\n",
    "\n",
    "    return rmse\n",
    "\n",
    "from LPM import LPM_filter\n",
    "\n",
    "def predict_and_print(rgb_img1, rgb_img2, matcher, img0_raw, img1_raw, points, points_patch, dataset1, dataset2,\n",
    "                      searching_window_w, searching_window_h, patch_w, patch_h,entropy, verbose=True, experiment=False,\n",
    "                      configs_ransac=None):\n",
    "    '''\n",
    "\n",
    "        This code performs an image matching task with the given matcher, which takes two raw images and outputs corresponding features. The code first resizes the two raw images to (640, 480) and converts them to torch tensors, normalizing them by dividing each pixel by 255. The two images are then passed to the matcher to get feature matches and confidence scores.\n",
    "        The code then computes two weighted average points based on the matches, one weighted by the confidence score and the other by a uniform weight. If the number of matches is greater than 0, the code returns the weighted average points and prints them as figures if verbose is set to True. The output figures are saved as \"LoFTR-colab-demo.pdf\".\n",
    "I       f there are no matches, the code returns None, None, None, None.\n",
    "'''\n",
    "\n",
    "    img0 = torch.from_numpy(img0_raw)[None][None].cuda().to(torch.float) / 255.\n",
    "    img1 = torch.from_numpy(img1_raw)[None][None].cuda().to(torch.float) / 255.\n",
    "\n",
    "    batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "    # Inference with LoFTR and get prediction\n",
    "    with torch.no_grad():\n",
    "\n",
    "        matcher.matcher(batch)\n",
    "        mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "        mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "        mconf = batch['mconf'].cpu().numpy()\n",
    "\n",
    "        #mkpts0 = np.array([])\n",
    "\n",
    "    results = []\n",
    "    if mkpts0.shape[0] > 4:\n",
    "\n",
    "        if configs_ransac is not None:\n",
    "\n",
    "            for conf in configs_ransac:\n",
    "                from skimage.measure import ransac\n",
    "\n",
    "                if conf['residual_threshold'] >= 0:\n",
    "\n",
    "                    model, inliers = ransac((mkpts0, mkpts1), AffineTransform, min_samples=conf['min_samples'],\n",
    "                                            residual_threshold=conf['residual_threshold'],\n",
    "                                            max_trials=conf['max_trials'])\n",
    "\n",
    "                   # mkpts0= mkpts0[inliers]\n",
    "                   # mkpts1= mkpts1[inliers]\n",
    "                   # mconf = mconf[inliers]\n",
    "\n",
    "                    #inliers = LPM_filter(mkpts0, mkpts1)\n",
    "                    n_inliers = np.sum(inliers)\n",
    "\n",
    "\n",
    "                    from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "                    if inliers is None or n_inliers is None:# or (n_inliers < threshold_inliers):\n",
    "                        conf.update({'rmse': -1, 'inliers': 0 if n_inliers is None else n_inliers})\n",
    "                    # results.append({'rmse':-1,'inliers':})\n",
    "                    else:\n",
    "                        gray = np.array(img0.cpu()).squeeze()\n",
    "                        corners = cv2.goodFeaturesToTrack(gray,100,0.01,20)\n",
    "                        #n_inliers=threshold_inliers\n",
    "                        corners = np.int0(corners)\n",
    "                        harris_near = np.zeros((mkpts0.shape[0])) + 500\n",
    "\n",
    "                        for match in range(len(mkpts0)):\n",
    "                            if inliers[match]:\n",
    "                                     arr = [abs(mkpts0[match][0]- corner.ravel()[0])+abs(mkpts0[match][1]- corner.ravel()[1] ) for corner in corners]\n",
    "                                     harris_near[match] = min(arr)\n",
    "                        new_order = harris_near.argsort()\n",
    "\n",
    "                        #inliers = inliers[new_order][:threshold_inliers]\n",
    "\n",
    "                        rmse = get_metrics(mkpts0[inliers], mkpts1[inliers], points, points_patch, dataset1, dataset2,\n",
    "                                           searching_window_w, searching_window_h, patch_w, patch_h, rgb_img1, rgb_img2,\n",
    "                                           mconf[inliers], verbose=verbose)\n",
    "                        conf.update({'rmse': rmse, 'inliers': n_inliers})\n",
    "                else:\n",
    "                    inliers = np.zeros_like(range(mkpts0.shape[0]))\n",
    "                    inliers[:] = True\n",
    "                    n_inliers = np.sum(inliers)\n",
    "                    gray = np.array(img0.cpu()).squeeze()\n",
    "                    corners = cv2.goodFeaturesToTrack(gray,25,0.01,10,\tuseHarrisDetector = True)\n",
    "                    #n_inliers=threshold_inliers\n",
    "                    corners = np.int0(corners)\n",
    "                    harris_near = np.zeros((mkpts0.shape[0])) + 500\n",
    "\n",
    "                    for match in range(len(mkpts0)):\n",
    "                        if inliers[match]:\n",
    "                                 arr = [abs(mkpts0[match][0]- corner.ravel()[0])+abs(mkpts0[match][1]- corner.ravel()[1] ) for corner in corners]\n",
    "                                 harris_near[match] = min(arr)\n",
    "\n",
    "                    new_order = harris_near.argsort()\n",
    "\n",
    "                    #inliers = inliers[new_order][:threshold_inliers]\n",
    "                    rmse = get_metrics(mkpts0[inliers],mkpts1[inliers], points, points_patch, dataset1, dataset2, searching_window_w,\n",
    "                                       searching_window_h, patch_w, patch_h, rgb_img1, rgb_img2, mconf[inliers], verbose=verbose)\n",
    "\n",
    "                    conf.update({'rmse': rmse, 'inliers': n_inliers})\n",
    "                results.append(conf)\n",
    "\n",
    "                if not( inliers is None or n_inliers is None):# or (n_inliers < threshold_inliers)):\n",
    "\n",
    "                    color = cm.jet(mconf[inliers], alpha=0.7)\n",
    "                    text = [\n",
    "                        str(conf),\n",
    "                        'Matches: {}'.format(len(mkpts0[inliers])),\n",
    "                    ]\n",
    "                    #print(mkpts0[inliers])\n",
    "                    abs_m0 = np.array([(x + points[1], y + points[0]) for x, y in mkpts0[inliers]])\n",
    "                    abs_m1 = np.array([(x - ((searching_window_h / 2) - (patch_h / 2)) + points_patch[1],\n",
    "                                        y - ((searching_window_w / 2) - (patch_w / 2)) + points_patch[0]) for x, y in\n",
    "                                       mkpts1[inliers]])\n",
    "                    if conf['residual_threshold'] <= 1:\n",
    "\n",
    "                        if verbose:\n",
    "                            fig = make_matching_figure(rgb_img1, rgb_img2, abs_m0, abs_m1, color, abs_m0, abs_m1,\n",
    "                                                       text)\n",
    "                        make_matching_figure(rgb_img1, rgb_img2, abs_m0, abs_m1, color, abs_m0, abs_m1, text,\n",
    "                                             path=\"ransac.pdf\" if conf['residual_threshold'] == 1 else \"pre_ransac.pdf\")\n",
    "\n",
    "        return results\n",
    "    return None\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from despekle.transform_main import TransSARV2\n",
    "import cv2\n",
    "\n",
    "loaddirec = \"model.pth\"\n",
    "save_path = \"./\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "if model_TransSAR is None:\n",
    "    model_TransSAR = TransSARV2()\n",
    "    model_TransSAR.to(device)\n",
    "    model_TransSAR = nn.DataParallel(model_TransSAR, device_ids=[0]).cuda()\n",
    "    model_TransSAR.load_state_dict(torch.load(loaddirec))\n",
    "    model_TransSAR.eval()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "'''\n",
    " It takes two images as input, performs image denoising, and then uses a matcher to find the similarity between a patch from the first image and the second image.\n",
    "\n",
    "The code uses a loop to run the processing for 50 times, for each iteration:\n",
    "\n",
    "It calls the function get_sample to get the images, points, and datasets.\n",
    "It denoises the raw images using 3 denoising methods (mean, bilateral, and lee_enhanced). If the denoising didn't produce a result, the code skips to the next iteration.\n",
    "The code calls the function predict_and_print to get the result of the matcher and to find the similarity between the patch and the search window.\n",
    "The code uses the result of the matcher to calculate the error in meters between the predicted position and the real position.\n",
    "The code draws circles on the original full map to show the predicted and real positions.\n",
    "The code displays the original full map if verbose is set to True.\n",
    "The code accumulates the error for each iteration and prints the final result, which includes the number of successful predictions and the mean error in meters.\n",
    "'''\n",
    "\n",
    "\n",
    "def lee_filter(img, size):\n",
    "    img_mean = uniform_filter(img, (size, size))\n",
    "    img_sqr_mean = uniform_filter(img ** 2, (size, size))\n",
    "    img_variance = img_sqr_mean - img_mean ** 2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "    return img_output\n",
    "\n",
    "\n",
    "def get_smoothed(file, savefile):\n",
    "    im_file = file\n",
    "    img = cv2.imread(im_file, 0)\n",
    "\n",
    "    #noisy_im = (np.float32(img)+1.0)/256.0\n",
    "\n",
    "    #x = np.float32(noisy_im)\n",
    "    #x = F.to_tensor(x)\n",
    "    #x = x.unsqueeze(0)\n",
    "\n",
    "    #pred_im = model_TransSAR(x)\n",
    "    #tmp = pred_im.detach().cpu().numpy()\n",
    "\n",
    "    #tmp = tmp.squeeze()\n",
    "    #tmp = tmp*256 -1\n",
    "\n",
    "    filepath_out = savefile\n",
    "\n",
    "    cv2.imwrite(filepath_out, img)\n",
    "\n",
    "\n",
    "def do_test(matcher_in, path0, path1, size_search=(640, 480), size_patch=(int(180 * 1.333333), int(180)),\n",
    "            configs_ransac=None, verbose=True):\n",
    "    rgb_img1, rgb_img2, points, points_patch_ref, points_patch, dataset1, dataset2 = get_sample(path0, path1,\n",
    "                                                                                                size_search, size_patch,\n",
    "                                                                                                verbose=verbose)\n",
    "    if rgb_img1 is None:\n",
    "        return None\n",
    "    img0_pth = \"./search_window.jpeg\"\n",
    "    img1_pth = \"./patch.jpeg\"\n",
    "    get_smoothed(\"search_window.jpeg\", \"search_window_s.jpeg\")\n",
    "    get_smoothed(\"patch.jpeg\", \"patch_s.jpeg\")\n",
    "    image_pair = [\"search_window_s.jpeg\", \"patch_s.jpeg\"]\n",
    "    img0_raw = cv2.imread(image_pair[0], cv2.IMREAD_GRAYSCALE)\n",
    "    img1_raw = cv2.imread(image_pair[1], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img0_denoised = img0_raw\n",
    "    img1_denoised = img1_raw\n",
    "    import skimage.measure\n",
    "    entropy_smoothed = skimage.measure.shannon_entropy(cv2.blur(img1_raw,(5,5)) )\n",
    "    results = predict_and_print(rgb_img1, rgb_img2, matcher_in, img0_denoised, img1_denoised, points, points_patch,\n",
    "                                dataset1, dataset2, size_search[0], size_search[1], size_patch[0], size_patch[1],entropy_smoothed,\n",
    "                                verbose=verbose, configs_ransac=configs_ransac)\n",
    "\n",
    "\n",
    "    entropy = skimage.measure.shannon_entropy(img1_raw)\n",
    "\n",
    "\n",
    "    return results,entropy,entropy_smoothed\n",
    "\n",
    "\n",
    "def get_list():\n",
    "    path_of_the_directory = './data/paired_sentinel/'\n",
    "    paths = []\n",
    "    for filename in os.listdir(path_of_the_directory):\n",
    "        f = os.path.join(path_of_the_directory, filename)\n",
    "        if not os.path.isfile(f):\n",
    "            lst = os.listdir(f)\n",
    "            if len(lst) > 1:\n",
    "                paths.append((os.path.join(path_of_the_directory, filename, lst[0]),\n",
    "                              os.path.join(path_of_the_directory, filename, lst[1])))\n",
    "    return paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 16:23:00.025 | INFO     | model.lightning_loftr:__init__:34 - Load './weights/outdoor-lite-SEA.ckpt' as pretrained checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1004, 1000)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 382.0396153241571, 'inliers': 1985}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 15.611044226923157, 'inliers': 9}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 5.432162613444202, 'inliers': 21}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 7.084455075582079, 'inliers': 78}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 18.187839386477236, 'inliers': 162}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 14.395024463221384, 'inliers': 341}\n",
      "(1020, 1017)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 241.21406815363667, 'inliers': 2302}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 1.6683150330336471, 'inliers': 571}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 2.077792556012628, 'inliers': 1319}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 2.4433339733512427, 'inliers': 1471}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 3.5992252180659445, 'inliers': 1684}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 5.2178984308398695, 'inliers': 1789}\n",
      "(1008, 1003)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 391.7609372091787, 'inliers': 2125}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 1.4752127072033931, 'inliers': 218}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 1.7667187565161295, 'inliers': 571}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 2.1873642752173943, 'inliers': 624}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 3.6248057846431325, 'inliers': 736}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 5.036620739312395, 'inliers': 783}\n",
      "(1017, 1013)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 579.6722202333809, 'inliers': 1965}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 2.9322532429127968, 'inliers': 24}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 4.222749498322035, 'inliers': 93}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 4.46898476444093, 'inliers': 191}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 5.1883737447859035, 'inliers': 297}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 8.33273522945125, 'inliers': 387}\n",
      "(1008, 1002)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 186.79895061478894, 'inliers': 2142}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 1.1154262513691402, 'inliers': 277}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 1.529002579625108, 'inliers': 650}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 2.1187328741238622, 'inliers': 695}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.9099039748502005, 'inliers': 972}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 8.407582136577743, 'inliers': 1226}\n",
      "(1037, 1033)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 312.9683107918502, 'inliers': 2123}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 1.4869313104205977, 'inliers': 277}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 2.0329396271822153, 'inliers': 686}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 2.8008262502706454, 'inliers': 865}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.481795665575641, 'inliers': 1165}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 6.877133581271985, 'inliers': 1333}\n",
      "(1010, 1006)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 314.6866681914211, 'inliers': 2165}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 1.7767130170570273, 'inliers': 220}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 2.3495228040090783, 'inliers': 645}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 3.1364465053083106, 'inliers': 1049}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.459687736565776, 'inliers': 1325}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 8.439014343809628, 'inliers': 1441}\n",
      "(1037, 1035)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 428.5115445841877, 'inliers': 2085}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 2.3235202913139243, 'inliers': 223}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 3.669004628006729, 'inliers': 528}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 3.4531223393954127, 'inliers': 923}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.90952570282306, 'inliers': 1187}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 6.866672441422441, 'inliers': 1333}\n",
      "(1015, 1012)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 374.2923490034482, 'inliers': 2194}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 1.405934796099637, 'inliers': 298}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 2.0014172970561623, 'inliers': 752}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 2.6308876551106777, 'inliers': 885}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.45056907740068, 'inliers': 1183}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 6.233149752711315, 'inliers': 1301}\n",
      "(1012, 1007)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 143.9752636227637, 'inliers': 2123}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 2.075679744206248, 'inliers': 394}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 2.608675086284815, 'inliers': 905}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 2.839974225825713, 'inliers': 1071}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 3.770926247158269, 'inliers': 1267}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 5.515565424478495, 'inliers': 1349}\n",
      "(1049, 1046)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 363.45146937173564, 'inliers': 2035}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 1.172044922393767, 'inliers': 84}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 1.4132742283515463, 'inliers': 270}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 1.813830417608885, 'inliers': 293}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 4.288478958874496, 'inliers': 397}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 7.562927864444552, 'inliers': 462}\n",
      "(1006, 1003)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n",
      "{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0, 'rmse': 367.54364448026956, 'inliers': 2135}\n",
      "{'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000, 'rmse': 4.242715429589489, 'inliers': 115}\n",
      "{'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000, 'rmse': 4.499635466572577, 'inliers': 361}\n",
      "{'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000, 'rmse': 3.504717532177912, 'inliers': 595}\n",
      "{'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000, 'rmse': 5.001284239092581, 'inliers': 897}\n",
      "{'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000, 'rmse': 7.633686389758988, 'inliers': 1041}\n",
      "(1026, 1022)\n",
      "sizeooo torch.Size([1, 1, 880, 720])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 23\u001B[0m\n\u001B[1;32m     19\u001B[0m     results,entropy,entropy_smoothed \u001B[38;5;241m=\u001B[39m do_test(matcher_in, path0, path1, size_search\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m640\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m240\u001B[39m, \u001B[38;5;241m480\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m240\u001B[39m),\n\u001B[1;32m     20\u001B[0m                       size_patch\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28mint\u001B[39m(\u001B[38;5;241m180\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1.333333\u001B[39m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m, \u001B[38;5;28mint\u001B[39m(\u001B[38;5;241m180\u001B[39m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m), verbose\u001B[38;5;241m=\u001B[39mverbose,\n\u001B[1;32m     21\u001B[0m                       configs_ransac\u001B[38;5;241m=\u001B[39mconfigs_ransac)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 23\u001B[0m     results,entropy,entropy_smoothed \u001B[38;5;241m=\u001B[39m \u001B[43mdo_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmatcher_in\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize_search\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m640\u001B[39;49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m240\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m480\u001B[39;49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m240\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m                      \u001B[49m\u001B[43msize_patch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m180\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.333333\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m180\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mconfigs_ransac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfigs_ransac\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m results \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(results)):\n",
      "Cell \u001B[0;32mIn[6], line 68\u001B[0m, in \u001B[0;36mdo_test\u001B[0;34m(matcher_in, path0, path1, size_search, size_patch, configs_ransac, verbose)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mskimage\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmeasure\u001B[39;00m\n\u001B[1;32m     67\u001B[0m entropy_smoothed \u001B[38;5;241m=\u001B[39m skimage\u001B[38;5;241m.\u001B[39mmeasure\u001B[38;5;241m.\u001B[39mshannon_entropy(cv2\u001B[38;5;241m.\u001B[39mblur(img1_raw,(\u001B[38;5;241m5\u001B[39m,\u001B[38;5;241m5\u001B[39m)) )\n\u001B[0;32m---> 68\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mpredict_and_print\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrgb_img1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrgb_img2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmatcher_in\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg0_denoised\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg1_denoised\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpoints\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpoints_patch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mdataset1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize_search\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize_search\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize_patch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize_patch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43mentropy_smoothed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfigs_ransac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfigs_ransac\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m entropy \u001B[38;5;241m=\u001B[39m skimage\u001B[38;5;241m.\u001B[39mmeasure\u001B[38;5;241m.\u001B[39mshannon_entropy(img1_raw)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m results,entropy,entropy_smoothed\n",
      "Cell \u001B[0;32mIn[4], line 116\u001B[0m, in \u001B[0;36mpredict_and_print\u001B[0;34m(rgb_img1, rgb_img2, matcher, img0_raw, img1_raw, points, points_patch, dataset1, dataset2, searching_window_w, searching_window_h, patch_w, patch_h, entropy, verbose, experiment, configs_ransac)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mskimage\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmeasure\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ransac\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conf[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresidual_threshold\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 116\u001B[0m     model, inliers \u001B[38;5;241m=\u001B[39m \u001B[43mransac\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmkpts0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmkpts1\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mAffineTransform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmin_samples\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mresidual_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mresidual_threshold\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mmax_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmax_trials\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m    \u001B[38;5;66;03m# mkpts0= mkpts0[inliers]\u001B[39;00m\n\u001B[1;32m    121\u001B[0m    \u001B[38;5;66;03m# mkpts1= mkpts1[inliers]\u001B[39;00m\n\u001B[1;32m    122\u001B[0m    \u001B[38;5;66;03m# mconf = mconf[inliers]\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \n\u001B[1;32m    124\u001B[0m     \u001B[38;5;66;03m#inliers = LPM_filter(mkpts0, mkpts1)\u001B[39;00m\n\u001B[1;32m    125\u001B[0m     n_inliers \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(inliers)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.10/site-packages/skimage/measure/fit.py:852\u001B[0m, in \u001B[0;36mransac\u001B[0;34m(data, model_class, min_samples, residual_threshold, is_data_valid, is_model_valid, max_trials, stop_sample_num, stop_residuals_sum, stop_probability, random_state, initial_inliers)\u001B[0m\n\u001B[1;32m    849\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m validate_data \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_data_valid(\u001B[38;5;241m*\u001B[39msamples):\n\u001B[1;32m    850\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m--> 852\u001B[0m success \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msamples\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[38;5;66;03m# backwards compatibility\u001B[39;00m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m success \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m success:\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.10/site-packages/skimage/transform/_geometric.py:703\u001B[0m, in \u001B[0;36mProjectiveTransform.estimate\u001B[0;34m(self, src, dst, weights)\u001B[0m\n\u001B[1;32m    700\u001B[0m n, d \u001B[38;5;241m=\u001B[39m src\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m    702\u001B[0m src_matrix, src \u001B[38;5;241m=\u001B[39m _center_and_normalize_points(src)\n\u001B[0;32m--> 703\u001B[0m dst_matrix, dst \u001B[38;5;241m=\u001B[39m \u001B[43m_center_and_normalize_points\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39mall(np\u001B[38;5;241m.\u001B[39misfinite(src_matrix \u001B[38;5;241m+\u001B[39m dst_matrix)):\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mfull((d \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, d \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m), np\u001B[38;5;241m.\u001B[39mnan)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.10/site-packages/skimage/transform/_geometric.py:58\u001B[0m, in \u001B[0;36m_center_and_normalize_points\u001B[0;34m(points)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124;03m\"\"\"Center and normalize image points.\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \n\u001B[1;32m     26\u001B[0m \u001B[38;5;124;03mThe points are transformed in a two-step procedure that is expressed\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     55\u001B[0m \n\u001B[1;32m     56\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     57\u001B[0m n, d \u001B[38;5;241m=\u001B[39m points\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m---> 58\u001B[0m centroid \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpoints\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m centered \u001B[38;5;241m=\u001B[39m points \u001B[38;5;241m-\u001B[39m centroid\n\u001B[1;32m     61\u001B[0m rms \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msqrt(np\u001B[38;5;241m.\u001B[39msum(centered \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m/\u001B[39m n)\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mmean\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432\u001B[0m, in \u001B[0;36mmean\u001B[0;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[1;32m   3429\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3430\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m mean(axis\u001B[38;5;241m=\u001B[39maxis, dtype\u001B[38;5;241m=\u001B[39mdtype, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m-> 3432\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_methods\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mean\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3433\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.10/site-packages/numpy/core/_methods.py:168\u001B[0m, in \u001B[0;36m_mean\u001B[0;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[1;32m    164\u001B[0m arr \u001B[38;5;241m=\u001B[39m asanyarray(a)\n\u001B[1;32m    166\u001B[0m is_float16_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m--> 168\u001B[0m rcount \u001B[38;5;241m=\u001B[39m \u001B[43m_count_reduce_items\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rcount \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m where \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m umr_any(rcount \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    170\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMean of empty slice.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;167;01mRuntimeWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.10/site-packages/numpy/core/_methods.py:72\u001B[0m, in \u001B[0;36m_count_reduce_items\u001B[0;34m(arr, axis, keepdims, where)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     71\u001B[0m     axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mrange\u001B[39m(arr\u001B[38;5;241m.\u001B[39mndim))\n\u001B[0;32m---> 72\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     73\u001B[0m     axis \u001B[38;5;241m=\u001B[39m (axis,)\n\u001B[1;32m     74\u001B[0m items \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "matcher_in = get_matcher(\"outdoor\", unlock=True)\n",
    "verbose = False\n",
    "random.seed(9)\n",
    "count_yes = 0\n",
    "test_for_each_pair = 1\n",
    "configs_ransac = [{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0},\n",
    "                  {'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000},\n",
    "                  {'min_samples': 4, 'residual_threshold': 3, 'max_trials': 10000},\n",
    "                  {'min_samples': 4, 'residual_threshold': 5, 'max_trials': 10000},\n",
    "                  {'min_samples': 4, 'residual_threshold': 10, 'max_trials': 10000},\n",
    "                  {'min_samples': 4, 'residual_threshold': 20, 'max_trials': 10000}]\n",
    "paths = get_list()\n",
    "\n",
    "metrics = [{'rmse': 0, 'inliers': 0, 'accepted_match': 0} for conf in range(len(configs_ransac))]\n",
    "\n",
    "for path0, path1 in paths:\n",
    "    for x in range(test_for_each_pair):\n",
    "        if random.randint(0, 1):\n",
    "            results,entropy,entropy_smoothed = do_test(matcher_in, path0, path1, size_search=(640+240, 480+240),\n",
    "                              size_patch=(int(180 * 1.333333) * 2, int(180) * 2), verbose=verbose,\n",
    "                              configs_ransac=configs_ransac)\n",
    "        else:\n",
    "            results,entropy,entropy_smoothed = do_test(matcher_in, path1, path0, size_search=(640+240, 480+240),\n",
    "                              size_patch=(int(180 * 1.333333) * 2, int(180) * 2), verbose=verbose,\n",
    "                              configs_ransac=configs_ransac)\n",
    "\n",
    "        if results is not None:\n",
    "\n",
    "            for i in range(len(results)):\n",
    "                if  results[i]['rmse'] >= 0:\n",
    "                    if i == 1:\n",
    "                        os.rename(\"ransac.pdf\",\n",
    "                                  \"ransac_conf_\" + str(results[i]['rmse']) + \"-\" + str(results[i]['inliers']) +\"-entropy_\"+str(entropy) +  str(os.path.basename(path0))+\".pdf\")\n",
    "                    if i == 0:\n",
    "                        os.rename(\"pre_ransac.pdf\",\n",
    "                                  \"pre_ransac_\" + str(results[i]['rmse']) + \"-\" + str(results[i]['inliers']) +\"-entropy_\"+str(entropy)  + str(os.path.basename(path0))+ \".pdf\")\n",
    "\n",
    "                    metrics[i]['rmse'] += results[i]['rmse']\n",
    "\n",
    "                    metrics[i]['inliers'] += results[i]['inliers']\n",
    "                    metrics[i]['accepted_match'] += 1\n",
    "                print(results[i])\n",
    "                #print(metrics[i])\n",
    "\n",
    "            count_yes += 1\n",
    "\n",
    "data = []\n",
    "for i in range(len(metrics)):\n",
    "    metrics[i]['rmse'] /= metrics[i]['accepted_match']\n",
    "    metrics[i]['inliers'] /= metrics[i]['accepted_match']\n",
    "\n",
    "    configs_ransac[i]['rmse'] = metrics[i]['rmse']\n",
    "    configs_ransac[i]['inliers'] = metrics[i]['inliers']\n",
    "    configs_ransac[i]['Accepted_match'] = metrics[i]['accepted_match']\n",
    "    configs_ransac[i]['total_match'] = test_for_each_pair * len(paths)\n",
    "    data.append(configs_ransac[i])\n",
    "    #print(\"Configuration:\",configs_ransac[i-1],\" Metrics:\",metrics[i], \" Accepted_match:\", metrics[i]['accepted_match'] ,\"/\", test_for_each_pair*len(paths))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_excel('players.xlsx')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_excel('players.xlsx')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
