{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIGuQbmAl3-q"
   },
   "source": [
    "# Readme: how to handle our Sentinel-1 ASC-DESC dataset\n",
    "\n",
    "###What you have: \n",
    "A list of big geotiff file (.tif) Pairs. Each image in the pair should roughly correspond to the same area as the other. Each geotiff can by read using rasterio or gdal, as you can see in the notebook.\n",
    "\n",
    "###What you should do: \n",
    "create single datapoints to train/test your models.\n",
    "\n",
    "###How to do it: \n",
    "For each big pair, get a datapoint by cropping a random patch from one side (this acts as the \"query\" patch, the one you want to look for) and cropping a patch around the same coordinates from the other side (this acts as the \"search\" patch, the one to want to search in). Add distortions: search patch has to be taken with a random offset around query patch (perhaps random but normally distributed around offset 0, which means query patch at the centre). Also add a bit of random scale and rotation.Try finding a 1km^2 query patch inside a 4km^2 search patch. \n",
    "\n",
    "###Some WARNINGS (not too sure about them)\n",
    "\n",
    "Warning: do not include every correspondence in the ground truth pixel correspondence list! you should only insert meaningful correspondences such as harris corners.\n",
    "\n",
    "Warning: maybe discard a candidate query altogether if it's not feature-rich enough. You can do this using pixel histograms (has to have more than n peaks -> use scikit learn), or maybe entropy. for example, if every pixel is around e.g. 150, this means that everything is gray and unmatchable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xrueqd_5HPm0",
    "outputId": "faa7755a-0d9c-4a33-9488-943c36621724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rasterio in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (1.3.5.post1)\r\n",
      "Requirement already satisfied: setuptools in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (65.6.3)\r\n",
      "Requirement already satisfied: snuggs>=1.4.1 in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (1.4.7)\r\n",
      "Requirement already satisfied: attrs in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (22.2.0)\r\n",
      "Requirement already satisfied: certifi in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (2022.12.7)\r\n",
      "Requirement already satisfied: numpy>=1.18 in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (1.23.4)\r\n",
      "Requirement already satisfied: click>=4.0 in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (8.1.3)\r\n",
      "Requirement already satisfied: affine in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (2.4.0)\r\n",
      "Requirement already satisfied: click-plugins in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (1.1.1)\r\n",
      "Requirement already satisfied: cligj>=0.5 in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from rasterio) (0.7.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in /home/fred/anaconda3/envs/pythonProject/lib/python3.10/site-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from scipy.ndimage import uniform_filter, variance\n",
    "from skimage.transform import AffineTransform\n",
    "\n",
    "from config.defaultmf import get_cfg_defaults\n",
    "from model.lightning_loftr import PL_LoFTR\n",
    "\n",
    "\n",
    "! pip install rasterio\n",
    "import rasterio as rio\n",
    "from rasterio import warp\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import argparse\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Need this to plot in HD\n",
    "# This takes up a lot of memory!\n",
    "matplotlib.rcParams['figure.dpi'] = 300\n",
    "\n",
    "if 'model_TransSAR' not in locals():\n",
    "    model_TransSAR = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "_IMltSlzD-C0"
   },
   "outputs": [],
   "source": [
    "def get_correspondence_multi(src_data, dst_data, src_row, src_col):\n",
    "    ''' Gets the pixel coordinates (dst_row,dst_col) in dst_data of a given pixel at (src_row,src_col) in src_data.\n",
    "        Warning: no handling of pixel being outside dst_data! You have to handle that yourself.\n",
    "        TODO: Should add a check to only get the correspondences with high enough cornerness on both sides\n",
    "        (because we should only insert meaningful correspondences in the list!).\n",
    "    '''\n",
    "    # Get geo coords of pixel from src dataset\n",
    "    X1, Y1 = rio.transform.xy(src_data.transform, src_row, src_col)\n",
    "    # print(\"Geographic coordinates in crs 1: \",X1,Y1)\n",
    "\n",
    "    if src_data.crs != dst_data.crs:\n",
    "        # convert coordinates in the crs of the dst dataset\n",
    "        X2, Y2 = warp.transform(src_data.crs, dst_data.crs, X1, Y1)\n",
    "    else:\n",
    "        # if the crs is the same, do nothing\n",
    "        X2, Y2 = X1, Y1\n",
    "\n",
    "    # Get corresponding px coords in dst dataset\n",
    "    # It still returns an index even if out of bounds\n",
    "    dst_row, dst_col = rio.transform.rowcol(dst_data.transform, X2, Y2)\n",
    "    # print(\"Corresponding pixel coordinates in image 2: \",dst_row,dst_col)\n",
    "    return dst_row, dst_col\n",
    "\n",
    "\n",
    "def get_correspondence(src_data, dst_data, src_row, src_col):\n",
    "    ''' Gets the pixel coordinates (dst_row,dst_col) in dst_data of a given pixel at (src_row,src_col) in src_data.\n",
    "        Warning: no handling of pixel being outside dst_data! You have to handle that yourself.\n",
    "        TODO: Should add a check to only get the correspondences with high enough cornerness on both sides\n",
    "        (because we should only insert meaningful correspondences in the list!).\n",
    "    '''\n",
    "    # Get geo coords of pixel from src dataset\n",
    "    X1, Y1 = src_data.xy(src_row, src_col)\n",
    "    X1, Y1 = X1 if type(X1) is list else [X1], Y1 if type(Y1) is list else [Y1]\n",
    "    #print(\"Geographic coordinates in crs 1: \",X1,Y1)\n",
    "\n",
    "    if src_data.crs != dst_data.crs:\n",
    "        # convert coordinates in the crs of the dst dataset\n",
    "        X2, Y2 = warp.transform(src_data.crs, dst_data.crs, X1, Y1)\n",
    "        X2 = X2[0]\n",
    "        Y2 = Y2[0]\n",
    "    else:\n",
    "        # if the crs is the same, do nothing\n",
    "        X2, Y2 = X1, Y1\n",
    "\n",
    "    # Get corresponding px coords in dst dataset\n",
    "    # It still returns an index even if out of bounds\n",
    "    dst_row, dst_col = dst_data.index(X2, Y2)\n",
    "    dst_row, dst_col = dst_row[0] if type(dst_row) is list else dst_row, dst_col[0] if type(\n",
    "        dst_col) is list else dst_col\n",
    "    if dst_row < 0 or dst_col < 0:\n",
    "        print(\"make the margin higher, the corresponding points are outside the image\")\n",
    "    #print(\"Corresponding pixel coordinates in image 2: \",dst_row,dst_col)\n",
    "    return dst_row, dst_col\n",
    "\n",
    "\n",
    "def get_datapoint(ref_data, query_data):\n",
    "    ''' TODO: Call this method on a pair of rasterio datasets. It will generate a random datapoint consisting\n",
    "        of a smaller SAR patch from the query dataset, and a bigger SAR patch from the reference dataset.\n",
    "        The search patch contains the area of the smaller patch, with a random offset.\n",
    "    '''\n",
    "    datapoint = None\n",
    "    return datapoint\n",
    "\n",
    "\n",
    "def is_good_patch(patch):\n",
    "    ''' TODO: Checks if the random query patch is sufficiently texture-rich to be used to train/test the matching model.\n",
    "        If the patch is too plain (e.g. sea or desert), returns False. This function should also be used at test time:\n",
    "        if a sensor image is too plain, there's no need to match it.\n",
    "        At test time, something similar should also be done on the reference side.\n",
    "    '''\n",
    "    good = None\n",
    "    return good\n",
    "\n",
    "\n",
    "def get_keypoints(patch):\n",
    "    ''' TODO: Use something like harris corner detection to get the list of ground truth correspondences.\n",
    "        In fact, you should not train on each pixel corresp, but you should select only meaningful corresp!\n",
    "        Harris peaks might be just strong speckle noise, but if you take strong ones you should be fine.\n",
    "    '''\n",
    "    keypoints = None\n",
    "    return keypoints\n",
    "\n",
    "\n",
    "def normalize_image(img):\n",
    "    ''' Normalize by clipping to 99th percentile and convert to uint8.\n",
    "        This clips strong speckle outliers and optimizes the brightness range\n",
    "    '''\n",
    "    p1 = np.nanpercentile(img, 99)\n",
    "    img = img.clip(0, p1)\n",
    "    img = (img - np.nanmin(img)) / (np.nanmax(img) - np.nanmin(img)) * 255\n",
    "    img = img.astype(np.uint8, copy=True)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    './data/paired_sentinel/Sentinel1-AD Dataset- Lat-0.015502064951149919Lon41.3552557262833/S1A_IW_GRDH_1SDV_20220808T060138_20220808T060203_044457_054E18_B16D.tif',\n",
    "    './data/paired_sentinel/Sentinel1-AD Dataset- Lat-0.015502064951149919Lon41.3552557262833/S1A_IW_GRDH_1SDV_20220814T175523_20220814T175548_044552_055140_2AEA.tif']\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def get_sample(tiff1_path, tiff2_path, search_window, patch_size, margin=40, random_seed=1, verbose=True,\n",
    "               random_rotation=0.03, random_zoom=0.03):\n",
    "    '''\n",
    "          1-Reads the first band of the TIFF files using the rio library and normalizes the image data.\n",
    "          2-Selects a random search window and patch location within the image using random number generators.\n",
    "          3-Calls the get_correspondence function on the selected locations to find the corresponding locations in the second image.\n",
    "          4-Converts the grayscale images to RGB format using OpenCV.\n",
    "          5-Creates centered patches from the RGB images by zero-padding the images and copying a portion of the original images to the patches.\n",
    "          6-Draws rectangles around the search window and patch in both images.\n",
    "          7-Saves the patch and search window as JPEG files.\n",
    "          8-Returns the processed RGB images, the points of the search window and patch, and the original rio datasets.\n",
    "\n",
    "    '''\n",
    "    dataset1 = rio.open(tiff1_path)\n",
    "    dataset2 = rio.open(tiff2_path)\n",
    "    img1 = dataset1.read(1)\n",
    "    img1 = normalize_image(img1)\n",
    "    img2 = dataset2.read(1)\n",
    "    img2 = normalize_image(img2)\n",
    "\n",
    "    img1 = np.swapaxes(img1, 0, 1)\n",
    "    img2 = np.swapaxes(img2, 0, 1)\n",
    "\n",
    "    search_window_w, search_window_h = search_window\n",
    "    patch_size_w, patch_size_h = patch_size\n",
    "\n",
    "    if img1.shape[0] - search_window_w - margin * 2 < 0 or img1.shape[1] - search_window_h - margin * 2 < 0:\n",
    "        print(\"margin + search windows is too big for the image:\", margin, \"*2 +\", (search_window_w, search_window_h),\n",
    "              \">\", img1.shape)\n",
    "        return None, None, None, None, None, None, None\n",
    "    lu = (margin + random.randint(0, img1.shape[0] - search_window_w - margin * 2),\n",
    "          margin + random.randint(0, img1.shape[1] - search_window_h - margin * 2))\n",
    "    lu_patch = (lu[0] + random.randint(0, search_window_w - patch_size_w),\n",
    "                lu[1] + random.randint(0, search_window_h - patch_size_h))\n",
    "\n",
    "    points_patch = lu_patch\n",
    "    points = lu\n",
    "    #print(points,\"points\",img1.shape)\n",
    "    points_ref = get_correspondence(dataset1, dataset2, lu[0], lu[1])\n",
    "    points_patch_ref = get_correspondence(dataset1, dataset2, lu_patch[0], lu_patch[1])\n",
    "\n",
    "    rgb_img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2RGB)\n",
    "    rgb_img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    patch_source = np.zeros((search_window_w, search_window_h), dtype=np.uint8)\n",
    "\n",
    "    patch_source[int(search_window_w / 2 - patch_size_w / 2):int(search_window_w / 2 + patch_size_w / 2),\n",
    "    int(search_window_h / 2 - patch_size_h / 2):int(search_window_h / 2 + patch_size_h / 2)] = img1[points_patch[0]:\n",
    "                                                                                                    points_patch[\n",
    "                                                                                                        0] + patch_size_w,\n",
    "                                                                                               points_patch[1]:\n",
    "                                                                                               points_patch[\n",
    "                                                                                                   1] + patch_size_h]\n",
    "\n",
    "    patch_dest = np.zeros((search_window_w, search_window_h), dtype=np.uint8)\n",
    "    patch_dest[int(search_window_w / 2 - patch_size_w / 2):int(search_window_w / 2 + patch_size_w / 2),\n",
    "    int(search_window_h / 2 - patch_size_h / 2):int(search_window_h / 2 + patch_size_h / 2)] = img2[points_patch_ref[0]:\n",
    "                                                                                                    points_patch_ref[\n",
    "                                                                                                        0] + patch_size_w,\n",
    "                                                                                               points_patch_ref[1]:\n",
    "                                                                                               points_patch_ref[\n",
    "                                                                                                   1] + patch_size_h]\n",
    "\n",
    "    search_window_source = img1[points[0]:points[0] + search_window_w, points[1]:points[1] + search_window_h]\n",
    "    search_window_dest = img2[points_ref[0]:points_ref[0] + search_window_w,\n",
    "                         points_ref[1]: points_ref[1] + search_window_h]\n",
    "\n",
    "\n",
    "\n",
    "    #draw searching windows\n",
    "\n",
    "    rgb_img1 = cv2.rectangle(rgb_img1, tuple(reversed(points)),\n",
    "                             (points[1] + search_window_h, points[0] + search_window_w), (0, 0, 255), 2)\n",
    "    rgb_img1 = cv2.rectangle(rgb_img1, tuple(reversed(points_patch)),\n",
    "                             (points_patch[1] + patch_size_h, points_patch[0] + patch_size_w), (255, 0, 0), 2)\n",
    "\n",
    "    #draw patch windows\n",
    "    rgb_img2 = cv2.rectangle(rgb_img2, tuple(reversed(points_ref)),\n",
    "                             (points_ref[1] + search_window_h, points_ref[0] + search_window_w), (0, 0, 255), 2)\n",
    "    rgb_img2 = cv2.rectangle(rgb_img2, tuple(reversed(points_patch_ref)),\n",
    "                             (points_patch_ref[1] + patch_size_h, points_patch_ref[0] + patch_size_w), (255, 0, 0), 2)\n",
    "\n",
    "    #print(rgb_img1.shape,search_window_source.shape)\n",
    "\n",
    "    rgb_img1 = np.swapaxes(rgb_img1, 0, 1)\n",
    "    rgb_img2 = np.swapaxes(rgb_img2, 0, 1)\n",
    "    search_window_source = np.swapaxes(search_window_source, 0, 1)\n",
    "    patch_source = np.swapaxes(patch_source, 0, 1)\n",
    "    search_window_dest = np.swapaxes(search_window_dest, 0, 1)\n",
    "    patch_dest = np.swapaxes(patch_dest, 0, 1)\n",
    "    if verbose:\n",
    "        fig, axes = plt.subplots(2, 3)\n",
    "        axes[0, 0].set_title('source image')\n",
    "        print(rgb_img1.shape)\n",
    "        axes[0, 0].imshow(PIL.ImageOps.invert(Image.fromarray(rgb_img1)))\n",
    "\n",
    "        axes[0, 1].set_title('search window source')\n",
    "        axes[0, 1].imshow(PIL.ImageOps.invert(Image.fromarray(cv2.cvtColor(search_window_source, cv2.COLOR_GRAY2RGB))))\n",
    "\n",
    "        axes[0, 2].set_title('patch source')\n",
    "        axes[0, 2].imshow(PIL.ImageOps.invert(Image.fromarray(cv2.cvtColor(patch_source, cv2.COLOR_GRAY2RGB))))\n",
    "\n",
    "        axes[1, 0].set_title('dest image')\n",
    "        axes[1, 0].imshow(PIL.ImageOps.invert(Image.fromarray(rgb_img2)))\n",
    "\n",
    "        axes[1, 1].set_title('search window dest')\n",
    "        axes[1, 1].imshow(PIL.ImageOps.invert(Image.fromarray(cv2.cvtColor(search_window_dest, cv2.COLOR_GRAY2RGB))))\n",
    "\n",
    "        axes[1, 2].set_title('patch dest')\n",
    "        axes[1, 2].imshow(PIL.ImageOps.invert(Image.fromarray(cv2.cvtColor(patch_dest, cv2.COLOR_GRAY2RGB))))\n",
    "\n",
    "        plt.show()\n",
    "    rgb_img1 = np.swapaxes(rgb_img1, 0, 1)\n",
    "    rgb_img2 = np.swapaxes(rgb_img2, 0, 1)\n",
    "    patch_dest = np.swapaxes(patch_dest, 0, 1)\n",
    "    search_window_source = np.swapaxes(search_window_source, 0, 1)\n",
    "\n",
    "    return patch_dest,search_window_source,rgb_img1, rgb_img2, points, points_patch_ref, points_patch, dataset1, dataset2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # init a costum parser which will be added into pl.Trainer parser\n",
    "    # check documentation: https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-flags\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        'data_cfg_path', type=str, default=\".\", help='data config path')\n",
    "    parser.add_argument(\n",
    "        '--ckpt_path', type=str, default=\"weights/indoor_large-SEA.ckpt\", help='path to the checkpoint')\n",
    "    parser.add_argument(\n",
    "        '--dump_dir', type=str, default=None, help=\"if set, the matching results will be dump to dump_dir\")\n",
    "    parser.add_argument(\n",
    "        '--profiler_name', type=str, default='inference', help='options: [inference, pytorch], or leave it unset')\n",
    "    parser.add_argument(\n",
    "        '--batch_size', type=int, default=1, help='batch_size per gpu')\n",
    "    parser.add_argument(\n",
    "        '--num_workers', type=int, default=2)\n",
    "    parser.add_argument(\n",
    "        '--thr', type=float, default=None, help='modify the coarse-level matching threshold.')\n",
    "\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "from plotting import make_matching_figure\n",
    "\n",
    "def get_matcher(image_type, unlock=False):\n",
    "    print(\"hello\")\n",
    "    #args = parse_args()\n",
    "    # init default-cfg and merge it with the main- and data-cfg\n",
    "    config = get_cfg_defaults()\n",
    "    #config.merge_from_file(args.data_cfg_path)\n",
    "    pl.seed_everything(config.TRAINER.SEED)  # reproducibility\n",
    "\n",
    "    # tune when testing\n",
    "    threshold = None\n",
    "    if threshold is not None:\n",
    "        config.LOFTR.MATCH_COARSE.THR = threshold\n",
    "\n",
    "    # lightning module\n",
    "\n",
    "    matcher = PL_LoFTR(config, pretrained_ckpt=\"./weights/outdoor-lite-SEA.ckpt\", dump_dir=\".\")\n",
    "\n",
    "    return matcher.cuda().eval()\n",
    "from skimage.measure import ransac\n",
    "\n",
    "\n",
    "def get_metrics(mkpts0_r, mkpts1_r, points, points_patch, dataset1, dataset2, searching_window_w, searching_window_h,\n",
    "                patch_w, patch_h, img0_raw, img1_raw, color, verbose=False):\n",
    "    # Extract x and y coordinates from mkpts0\n",
    "    x0, y0 = zip(*mkpts0_r)\n",
    "\n",
    "    # Apply translation to the coordinates\n",
    "    x_conv, y_conv = get_correspondence_multi(dataset1, dataset2, [x + points[1] for x in x0],\n",
    "                                              [y + points[0] for y in y0])\n",
    "\n",
    "    # Convert the translated coordinates back to tuples\n",
    "    mk0 = list(zip(x_conv, y_conv))\n",
    "\n",
    "    # Calculate RMSE\n",
    "    rmse = 0\n",
    "    for (x1, y1), (x2, y2) in zip(mk0, mkpts1_r):\n",
    "        y2 -= (searching_window_w / 2) - (patch_w / 2)\n",
    "        x2 -= (searching_window_h / 2) - (patch_h / 2)\n",
    "        y2 += points_patch[0]\n",
    "        x2 += points_patch[1]\n",
    "\n",
    "        rmse += ((x1 - x2) ** 2 + (y1 - y2) ** 2)\n",
    "\n",
    "    # Normalize the errors by the number of keypoints\n",
    "    num_kpts = len(mk0)\n",
    "    rmse = (rmse / num_kpts) ** (\n",
    "            1 / 2)  # like in \"A Transformer-Based Coarse-to-Fine Wide-Swath SAR Image Registration Method under Weak Texture Conditions\"\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def cornerness_friendly(gray, points, inliers,random_select=False):\n",
    "    corners = cv2.goodFeaturesToTrack(gray, 100, 0.01, 15)\n",
    "\n",
    "    corners = np.int0(corners)\n",
    "    harris_near = np.zeros((points.shape[0])) + 500\n",
    "\n",
    "    for match in range(len(points)):\n",
    "        if inliers[match]:\n",
    "            arr = [abs(points[match][0] - corner.ravel()[0]) + abs(points[match][1] - corner.ravel()[1]) for corner in\n",
    "                   corners]\n",
    "            harris_near[match] = min(arr)\n",
    "    result= harris_near.argsort()\n",
    "    if random_select:\n",
    "      random.shuffle(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def postproces_and_metric(conf, mkpts0, mkpts1, img0, dataset1, dataset2, points, points_patch, searching_window_w,\n",
    "                          searching_window_h, patch_w, patch_h, rgb_img1, rgb_img2, mconf, threshold_inliers, random_select,\n",
    "                          save_pdf=True):\n",
    "    if conf['residual_threshold'] >= 0:\n",
    "        import time\n",
    "\n",
    "        # get the start time\n",
    "        #st = time.time()\n",
    "        #model, inliers = ransac((mkpts0, mkpts1), AffineTransform, min_samples=conf['min_samples'],\n",
    "        #                        residual_threshold=conf['residual_threshold'],\n",
    "        #                        max_trials=conf['max_trials'])\n",
    "        #et = time.time()\n",
    "\n",
    "        # get the execution time\n",
    "        #elapsed_time = et - st\n",
    "        #print('Ransac Execution time:', elapsed_time, 'seconds inliers:', np.sum(inliers))\n",
    "\n",
    "        import pymagsac\n",
    "\n",
    "\n",
    "\n",
    "        #st = time.time()\n",
    "        order = (mconf).argsort()\n",
    "        mkpts0 = mkpts0[order]\n",
    "        mkpts1 = mkpts1[order]\n",
    "        mconf = mconf[order]\n",
    "\n",
    "        probabilities = mconf+ min(mconf)\n",
    "        probabilities  /= max(probabilities )\n",
    "\n",
    "        correspondences = np.float32([ np.concatenate(( mkpts0[m] , mkpts1[m] )) for m in range(len(mkpts0))]).reshape(-1,4)\n",
    "        H, inliers = pymagsac.findHomography(\n",
    "        np.ascontiguousarray(correspondences),\n",
    "        searching_window_w, searching_window_h, searching_window_w, searching_window_h,\n",
    "        probabilities = probabilities ,\n",
    "        sampler = 4,\n",
    "        #conf = 0.,\n",
    "        use_magsac_plus_plus = False,\n",
    "        min_iters=20000,\n",
    "        sigma_th = float(conf['residual_threshold']))\n",
    "        #print(mask)\n",
    "        #et = time.time()\n",
    "\n",
    "        # get the execution time\n",
    "        #elapsed_time = et - st\n",
    "        #print('Magsac++ Execution time:', elapsed_time, 'seconds inliers:',np.sum(mask))\n",
    "\n",
    "    else:\n",
    "        inliers = np.zeros_like(range(mkpts0.shape[0]))\n",
    "        inliers[:] = True\n",
    "\n",
    "\n",
    "    threshold_inliers = threshold_inliers if threshold_inliers >= 0 else np.sum(inliers)\n",
    "    n_inliers = np.sum(inliers)\n",
    "\n",
    "    if inliers is None or n_inliers is None or n_inliers == 0 or (n_inliers < threshold_inliers) :\n",
    "        conf.update({'rmse': -1, 'inliers': 0 if n_inliers is None else n_inliers})\n",
    "    else:\n",
    "        gray = np.array(img0.cpu()).squeeze()\n",
    "        new_order = cornerness_friendly(gray, mkpts0, inliers,random_select)\n",
    "\n",
    "\n",
    "        inliers = inliers[new_order]\n",
    "        print(mkpts0.shape,mkpts0[new_order][inliers][:threshold_inliers].shape)\n",
    "\n",
    "        rmse = get_metrics(mkpts0[new_order][inliers][:threshold_inliers], mkpts1[new_order][inliers][:threshold_inliers], points,\n",
    "                           points_patch, dataset1, dataset2,\n",
    "                           searching_window_w, searching_window_h, patch_w, patch_h, rgb_img1, rgb_img2,\n",
    "                           mconf[new_order][inliers][:threshold_inliers], verbose=verbose)\n",
    "        conf.update({'rmse': rmse, 'inliers': n_inliers})\n",
    "\n",
    "        if save_pdf:\n",
    "            color = cm.jet(mconf[new_order][inliers][:threshold_inliers], alpha=0.7)\n",
    "            text = [\n",
    "                str(conf),\n",
    "                'Matches: {}'.format(len(mkpts0[new_order][inliers][:threshold_inliers])),\n",
    "            ]\n",
    "            abs_m0 = np.array([(x + points[1], y + points[0]) for x, y in mkpts0[new_order][inliers][:threshold_inliers]])\n",
    "            abs_m1 = np.array([(x - ((searching_window_h / 2) - (patch_h / 2)) + points_patch[1],\n",
    "                                y - ((searching_window_w / 2) - (patch_w / 2)) + points_patch[0]) for x, y in\n",
    "                               mkpts1[new_order][inliers][:threshold_inliers]])\n",
    "            if conf['residual_threshold'] <= 0.5:\n",
    "\n",
    "                if verbose:\n",
    "                    fig = make_matching_figure(rgb_img1, rgb_img2, abs_m0, abs_m1, color, abs_m0, abs_m1,\n",
    "                                               text)\n",
    "                make_matching_figure(rgb_img1, rgb_img2, abs_m0, abs_m1, color, abs_m0, abs_m1, text,\n",
    "                                     path=\"magsacpp\"+str(conf['residual_threshold']) +\"_rmse_\"+str(rmse)+\"_inliers_\" + str(n_inliers) +\".pdf\")\n",
    "    return conf\n",
    "\n",
    "\n",
    "def predict_and_print(rgb_img1, rgb_img2, matcher, img0_raw, img1_raw, points, points_patch, dataset1, dataset2,\n",
    "                      searching_window_w, searching_window_h, patch_w, patch_h, configs_ransac ,threshold_inliers,random_select =False):\n",
    "    '''\n",
    "        This code performs an image matching task with the given matcher, which takes two raw images and outputs corresponding features. The code first resizes the two raw images to (640, 480) and converts them to torch tensors, normalizing them by dividing each pixel by 255. The two images are then passed to the matcher to get feature matches and confidence scores.\n",
    "        The code then computes two weighted average points based on the matches, one weighted by the confidence score and the other by a uniform weight. If the number of matches is greater than 0, the code returns the weighted average points and prints them as figures if verbose is set to True. The output figures are saved as \"LoFTR-colab-demo.pdf\".\n",
    "I       f there are no matches, the code returns None, None, None, None.\n",
    "'''\n",
    "\n",
    "    img0 = torch.from_numpy(img0_raw)[None][None].cuda().to(torch.float) / 255.\n",
    "    img1 = torch.from_numpy(img1_raw)[None][None].cuda().to(torch.float) / 255.\n",
    "\n",
    "    batch = {'image0': img0, 'image1': img1}\n",
    "\n",
    "    # Inference with LoFTR and get prediction\n",
    "    with torch.no_grad():\n",
    "\n",
    "        matcher.matcher(batch)\n",
    "        mkpts0 = batch['mkpts0_f'].cpu().numpy()\n",
    "        mkpts1 = batch['mkpts1_f'].cpu().numpy()\n",
    "        mconf = batch['mconf'].cpu().numpy()\n",
    "\n",
    "    if mkpts0.shape[0] > 0:\n",
    "        return [postproces_and_metric(conf, mkpts0, mkpts1, img0, dataset1, dataset2, points, points_patch,\n",
    "                                           searching_window_w, searching_window_h, patch_w, patch_h, rgb_img1, rgb_img2,\n",
    "                                           mconf, threshold_inliers,random_select) for conf in configs_ransac]\n",
    "\n",
    "    return None\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "\n",
    "loaddirec = \"model.pth\"\n",
    "save_path = \"./\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " It takes two images as input, performs image denoising, and then uses a matcher to find the similarity between a patch from the first image and the second image.\n",
    "\n",
    "The code uses a loop to run the processing for 50 times, for each iteration:\n",
    "\n",
    "It calls the function get_sample to get the images, points, and datasets.\n",
    "It denoises the raw images using 3 denoising methods (mean, bilateral, and lee_enhanced). If the denoising didn't produce a result, the code skips to the next iteration.\n",
    "The code calls the function predict_and_print to get the result of the matcher and to find the similarity between the patch and the search window.\n",
    "The code uses the result of the matcher to calculate the error in meters between the predicted position and the real position.\n",
    "The code draws circles on the original full map to show the predicted and real positions.\n",
    "The code displays the original full map if verbose is set to True.\n",
    "The code accumulates the error for each iteration and prints the final result, which includes the number of successful predictions and the mean error in meters.\n",
    "'''\n",
    "\n",
    "\n",
    "def lee_filter(img, size):\n",
    "    img_mean = uniform_filter(img, (size, size))\n",
    "    img_sqr_mean = uniform_filter(img ** 2, (size, size))\n",
    "    img_variance = img_sqr_mean - img_mean ** 2\n",
    "\n",
    "    overall_variance = variance(img)\n",
    "\n",
    "    img_weights = img_variance / (img_variance + overall_variance)\n",
    "    img_output = img_mean + img_weights * (img - img_mean)\n",
    "    return img_output\n",
    "\n",
    "\n",
    "def do_test(matcher_in, path0, path1, size_search=(640, 480), size_patch=(int(180 * 1.333333), int(180)),threshold_inliers=25,\n",
    "            configs_ransac=None, verbose=True,random_select=False):\n",
    "    patch_dest,search_window_source, rgb_img1, rgb_img2, points, points_patch_ref, points_patch, dataset1, dataset2 = get_sample(path0, path1,\n",
    "                                                                                                size_search, size_patch,\n",
    "                                                                                                verbose=verbose)\n",
    "    if rgb_img1 is None:\n",
    "        return None\n",
    "\n",
    "    import skimage.measure\n",
    "\n",
    "    results = predict_and_print(rgb_img1, rgb_img2, matcher_in, search_window_source, patch_dest, points, points_patch,\n",
    "                                dataset1, dataset2, size_search[0], size_search[1], size_patch[0], size_patch[1]\n",
    "                                , configs_ransac,threshold_inliers,random_select\n",
    "                               )\n",
    "\n",
    "    entropy = skimage.measure.shannon_entropy(patch_dest)\n",
    "\n",
    "    return results, entropy\n",
    "\n",
    "\n",
    "def get_list():\n",
    "    path_of_the_directory = './data/UAVSAR/'\n",
    "    paths = []\n",
    "    for filename in os.listdir(path_of_the_directory):\n",
    "        f = os.path.join(path_of_the_directory, filename)\n",
    "        if not os.path.isfile(f):\n",
    "            lst = os.listdir(f)\n",
    "            if len(lst) > 1:\n",
    "                paths.append((os.path.join(path_of_the_directory, filename, lst[0]),\n",
    "                              os.path.join(path_of_the_directory, filename, lst[1])))\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 66\n",
      "2023-03-21 17:15:52.941 | INFO     | model.lightning_loftr:__init__:34 - Load './weights/outdoor-lite-SEA.ckpt' as pretrained checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/fred/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/data/UAVSAR/pair_98/1.jpg', '/home/fred/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/data/UAVSAR/pair_98/1_tf_crs.npy', '/home/fred/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/data/UAVSAR/pair_98/2.jpg', '/home/fred/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/data/UAVSAR/pair_98/2_llh.npy']\n",
      "['/home/fred/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/data/UAVSAR/pair_98/1.jpg', '/home/fred/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/data/UAVSAR/pair_98/1_tf_crs.npy', '/home/fred/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/data/UAVSAR/pair_98/2.jpg', '/home/fred/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/data/UAVSAR/pair_98/2_llh.npy']\n",
      "torch.Size([1, 1, 1200, 1200]) torch.Size([1, 1, 800, 800])\n",
      "tensor([[[[0.4000, 0.3725, 0.4392,  ..., 0.5922, 0.4510, 0.5843],\n",
      "          [0.4157, 0.5059, 0.4784,  ..., 0.5608, 0.4667, 0.6824],\n",
      "          [0.2941, 0.3765, 0.6196,  ..., 0.4118, 0.5333, 0.9020],\n",
      "          ...,\n",
      "          [0.3725, 0.3373, 0.3020,  ..., 0.7216, 0.5255, 0.3882],\n",
      "          [0.3333, 0.1725, 0.2275,  ..., 0.4157, 0.3804, 0.3098],\n",
      "          [0.2588, 0.2549, 0.4863,  ..., 0.4941, 0.6941, 0.5216]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[[0.3490, 0.5686, 0.5020,  ..., 0.3686, 0.4235, 0.2667],\n",
      "          [0.6745, 0.4471, 0.5059,  ..., 0.1765, 0.1137, 0.4078],\n",
      "          [0.7373, 0.4824, 0.4980,  ..., 0.0902, 0.0392, 0.3725],\n",
      "          ...,\n",
      "          [0.5765, 0.6627, 0.6941,  ..., 0.6745, 0.7490, 0.5647],\n",
      "          [0.8196, 0.7882, 0.7373,  ..., 0.3020, 0.3490, 0.5294],\n",
      "          [0.8000, 0.7686, 0.7686,  ..., 0.3137, 0.3176, 0.5333]]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "split_size can only be 0 if dimension size is 0, but got dimension size of 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[63], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(paths)\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m#p0, p1, =  \"1.jpg\", \"2.jpg\"(path0, path1) if \"llh\" in path0 else (path0, path1)\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m#llh_path\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m results, entropy \u001B[38;5;241m=\u001B[39m \u001B[43mdo_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmatcher_in\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpaths\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize_search\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m700\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m708\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43msize_patch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m160\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m168\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m                                               \u001B[49m\u001B[43mconfigs_ransac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfigs_ransac\u001B[49m\u001B[43m,\u001B[49m\u001B[43mthreshold_inliers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m25\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mrandom_select\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m results \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     29\u001B[0m       \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(results)):\n",
      "Cell \u001B[0;32mIn[62], line 55\u001B[0m, in \u001B[0;36mdo_test\u001B[0;34m(matcher_in, paths, size_search, size_patch, threshold_inliers, configs_ransac, verbose, random_select)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m#if rgb_img1 is None:\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m#    return None\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mskimage\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmeasure\u001B[39;00m\n\u001B[0;32m---> 55\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mpredict_and_print\u001B[49m\u001B[43m(\u001B[49m\u001B[43msearch_window_source\u001B[49m\u001B[43m,\u001B[49m\u001B[43mpatch_dest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmatcher_in\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mllh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize_search\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize_search\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize_patch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize_patch\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m                            \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfigs_ransac\u001B[49m\u001B[43m,\u001B[49m\u001B[43mthreshold_inliers\u001B[49m\u001B[43m,\u001B[49m\u001B[43mrandom_select\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m                           \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m entropy \u001B[38;5;241m=\u001B[39m skimage\u001B[38;5;241m.\u001B[39mmeasure\u001B[38;5;241m.\u001B[39mshannon_entropy(patch_dest)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m results, entropy\n",
      "Cell \u001B[0;32mIn[60], line 211\u001B[0m, in \u001B[0;36mpredict_and_print\u001B[0;34m(img0_raw, img1_raw, matcher, llh, crs, trans, searching_window_w, searching_window_h, patch_w, patch_h, configs_ransac, threshold_inliers, random_select)\u001B[0m\n\u001B[1;32m    208\u001B[0m \u001B[38;5;66;03m# Inference with LoFTR and get prediction\u001B[39;00m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 211\u001B[0m     \u001B[43mmatcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmatcher\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    212\u001B[0m     mkpts0 \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmkpts0_f\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    213\u001B[0m     mkpts1 \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmkpts1_f\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/model/matchformer.py:36\u001B[0m, in \u001B[0;36mMatchformer.forward\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     34\u001B[0m     (feat_c0, feat_c1),(feat_f0, feat_f1) \u001B[38;5;241m=\u001B[39m feats_c\u001B[38;5;241m.\u001B[39msplit(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbs\u001B[39m\u001B[38;5;124m'\u001B[39m]),feats_f\u001B[38;5;241m.\u001B[39msplit(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbs\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 36\u001B[0m     (feat_c0, feat_f0), (feat_c1, feat_f1) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackbone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage0\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackbone(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage1\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     38\u001B[0m data\u001B[38;5;241m.\u001B[39mupdate({\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhw0_c\u001B[39m\u001B[38;5;124m'\u001B[39m: feat_c0\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m:], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhw1_c\u001B[39m\u001B[38;5;124m'\u001B[39m: feat_c1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m:],\n\u001B[1;32m     40\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhw0_f\u001B[39m\u001B[38;5;124m'\u001B[39m: feat_f0\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m:], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhw1_f\u001B[39m\u001B[38;5;124m'\u001B[39m: feat_f1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m:]\n\u001B[1;32m     41\u001B[0m })\n\u001B[1;32m     43\u001B[0m feat_c0 \u001B[38;5;241m=\u001B[39m rearrange(feat_c0, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mn c h w -> n (h w) c\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/model/backbone/match_SEA_lite.py:264\u001B[0m, in \u001B[0;36mMatchformer_SEA_lite.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;66;03m# stage 1 # 1/4        \u001B[39;00m\n\u001B[0;32m--> 264\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAttentionBlock1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    265\u001B[0m     out1 \u001B[38;5;241m=\u001B[39m x\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;66;03m# stage 2 # 1/8\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/model/backbone/match_SEA_lite.py:201\u001B[0m, in \u001B[0;36mAttentionBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    199\u001B[0m x, H, W  \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_embed(x)\n\u001B[1;32m    200\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblock):\n\u001B[0;32m--> 201\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mblk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43mH\u001B[49m\u001B[43m,\u001B[49m\u001B[43mW\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    202\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x)\n\u001B[1;32m    203\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mreshape(B, H, W, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/model/backbone/match_SEA_lite.py:143\u001B[0m, in \u001B[0;36mBlock.forward\u001B[0;34m(self, x, H, W)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, H, W):\n\u001B[0;32m--> 143\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_path(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mH\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    144\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_path(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x), H, W))\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/LoFTR-master/MatchFormer/MatchFormer-main/model/backbone/match_SEA_lite.py:79\u001B[0m, in \u001B[0;36mAttention.forward\u001B[0;34m(self, x, H, W)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;66;03m#cross attention\u001B[39;00m\n\u001B[1;32m     78\u001B[0m q \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq(x)\u001B[38;5;241m.\u001B[39mreshape(B, N, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads, C \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n\u001B[0;32m---> 79\u001B[0m q1,q2 \u001B[38;5;241m=\u001B[39m \u001B[43mq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMiniB\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msr_ratio \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     82\u001B[0m     x_ \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mreshape(B, C, H, W)\n",
      "File \u001B[0;32m~/anaconda3/envs/pythonProject/lib/python3.10/site-packages/torch/_tensor.py:787\u001B[0m, in \u001B[0;36mTensor.split\u001B[0;34m(self, split_size, dim)\u001B[0m\n\u001B[1;32m    784\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    786\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(split_size, \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m--> 787\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    789\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_VF\u001B[38;5;241m.\u001B[39msplit_with_sizes(\u001B[38;5;28mself\u001B[39m, split_size, dim)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: split_size can only be 0 if dimension size is 0, but got dimension size of 1"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "matcher_in = get_matcher(\"outdoor\", unlock=True)\n",
    "verbose = False\n",
    "random.seed(9)\n",
    "count_yes = 0\n",
    "test_for_each_pair = 1\n",
    "configs_ransac = [{'min_samples': 0, 'residual_threshold': -1, 'max_trials': 0},\n",
    "                  {'min_samples': 4, 'residual_threshold': 0.5, 'max_trials': 10000},\n",
    "                  {'min_samples': 4, 'residual_threshold': 1, 'max_trials': 10000},\n",
    "                  {'min_samples': 4, 'residual_threshold': 1.5, 'max_trials': 10000},\n",
    "                  {'min_samples': 4, 'residual_threshold': 2, 'max_trials': 10000},\n",
    "                  {'min_samples': 4, 'residual_threshold': 2.5, 'max_trials': 10000}]\n",
    "paths = get_list()\n",
    "\n",
    "metrics = [{'rmse': 0, 'inliers': 0, 'accepted_match': 0} for conf in range(len(configs_ransac))]\n",
    "\n",
    "for path0, path1 in paths:\n",
    "    for x in range(test_for_each_pair):\n",
    "        p0, p1 =  (path0, path1) if random.randint(0, 1) else (path0, path1)\n",
    "\n",
    "        results, entropy = do_test(matcher_in, path0, path1, size_search=(640 + 240, 480 + 240),\n",
    "                                                     size_patch=(int(180 * 1.333333) * 2, int(180) * 2),\n",
    "                                                     verbose=verbose,\n",
    "                                                     configs_ransac=configs_ransac,threshold_inliers=25,random_select=False)\n",
    "\n",
    "\n",
    "        if results is not None:\n",
    "\n",
    "            for i in range(len(results)):\n",
    "                if results[i]['rmse'] >= 0:\n",
    "\n",
    "\n",
    "                    metrics[i]['rmse'] += results[i]['rmse']\n",
    "                    metrics[i]['inliers'] += results[i]['inliers']\n",
    "                    metrics[i]['accepted_match'] += 1\n",
    "                print(results[i])\n",
    "                #print(metrics[i])\n",
    "\n",
    "            count_yes += 1\n",
    "\n",
    "data = []\n",
    "for i in range(len(metrics)):\n",
    "    metrics[i]['rmse'] /= metrics[i]['accepted_match']\n",
    "    metrics[i]['inliers'] /= metrics[i]['accepted_match']\n",
    "\n",
    "    configs_ransac[i]['rmse'] = metrics[i]['rmse']\n",
    "    configs_ransac[i]['inliers'] = metrics[i]['inliers']\n",
    "    configs_ransac[i]['Accepted_match'] = metrics[i]['accepted_match']\n",
    "    configs_ransac[i]['total_match'] = test_for_each_pair * len(paths)\n",
    "    data.append(configs_ransac[i])\n",
    "    #print(\"Configuration:\",configs_ransac[i-1],\" Metrics:\",metrics[i], \" Accepted_match:\", metrics[i]['accepted_match'] ,\"/\", test_for_each_pair*len(paths))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_excel('players.xlsx')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_excel('players.xlsx')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
